{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Signature - Generate Watermarked Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruixin/anaconda3/envs/stable-signature/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module 'xformers'. Proceeding without it.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "from typing import Callable, Iterable\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import utils\n",
    "import utils_img\n",
    "import utils_model\n",
    "\n",
    "sys.path.append('src')\n",
    "from ldm.models.autoencoder import AutoencoderKL\n",
    "from ldm.models.diffusion.ddpm import LatentDiffusion\n",
    "from loss.loss_provider import LossProvider\n",
    "\n",
    "import gc         # garbage collect library\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    '''\n",
    "    Delete unused tensors on GPU to reduce OOM issues. \n",
    "    '''\n",
    "    try: \n",
    "        del ldm_decoder\n",
    "    except: \n",
    "        pass\n",
    "    try:\n",
    "        del ldm_ae\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        del msg_extractor\n",
    "    except: \n",
    "        pass\n",
    "    try: \n",
    "        del img\n",
    "    except:\n",
    "        pass\n",
    "    try: \n",
    "        del imgs\n",
    "    except:\n",
    "        pass\n",
    "    try: \n",
    "        del targets\n",
    "    except:\n",
    "        pass\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained LDM \n",
    "\n",
    "Load the pretrained Stable Diffusion model [configuration](https://github.com/Stability-AI/stablediffusion/blob/main/configs/stable-diffusion/v2-inference.yaml) and [checkpoint](\n",
    "https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.ckpt).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from sd/v2-1_512-ema-pruned.ckpt\n",
      "Global Step: 220000\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 865.91 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoencoderKL(\n",
       "  (encoder): Encoder(\n",
       "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down): ModuleList(\n",
       "      (0): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (downsample): Downsample(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (1): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (downsample): Downsample(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (2): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (downsample): Downsample(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "        )\n",
       "      )\n",
       "      (3): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "      )\n",
       "    )\n",
       "    (mid): Module(\n",
       "      (block_1): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (attn_1): AttnBlock(\n",
       "        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (block_2): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (mid): Module(\n",
       "      (block_1): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (attn_1): AttnBlock(\n",
       "        (norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (block_2): ResnetBlock(\n",
       "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (up): ModuleList(\n",
       "      (0): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (2): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "      )\n",
       "      (1): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (2): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (upsample): Upsample(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (2): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (2): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (upsample): Upsample(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (3): Module(\n",
       "        (block): ModuleList(\n",
       "          (0): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "          (2): ResnetBlock(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (attn): ModuleList()\n",
       "        (upsample): Upsample(\n",
       "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (loss): Identity()\n",
       "  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldm_config = \"sd/v2-inference.yaml\"\n",
    "ldm_ckpt = \"sd/v2-1_512-ema-pruned.ckpt\"\n",
    "config = OmegaConf.load(f\"{ldm_config}\")\n",
    "ldm_ae: LatentDiffusion = utils_model.load_model_from_config(config, ldm_ckpt)\n",
    "ldm_ae: AutoencoderKL = ldm_ae.first_stage_model\n",
    "ldm_ae.eval()\n",
    "ldm_ae.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader\n",
    "\n",
    "Images are loaded from the given directory, then resized (256x256) and cropped to standardize image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "val_dir = 'data/val'\n",
    "train_dir = 'data/train'\n",
    "batch_size = 1     # batch size = 1 to output single images\n",
    "key = \"key20\"\n",
    "key_dir = f'data/{key}'\n",
    "\n",
    "vqgan_transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.CenterCrop(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    utils_img.normalize_vqgan,\n",
    "    ])\n",
    "\n",
    "key_loader = utils.get_dataloader(key_dir, vqgan_transform, batch_size, num_imgs=100, shuffle=False, num_workers=4, collate_fn=None)\n",
    "#train_loader = utils.get_dataloader(train_dir, vqgan_transform, batch_size, num_imgs=500, shuffle=False, num_workers=4, collate_fn=None)\n",
    "#val_loader = utils.get_dataloader(val_dir, vqgan_transform, batch_size, num_imgs=1000, shuffle=False, num_workers=4, collate_fn=None)\n",
    "vqgan_to_imnet = transforms.Compose([utils_img.unnormalize_vqgan, utils_img.normalize_img])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load decoder\n",
    "\n",
    "Copies the pretrained autoencoder and replaces the weights of the decoder with the finetuned decoder that had been generated using the code: \n",
    "\n",
    "`python finetune_ldm_decoder.py --train_dir data/train --val_dir data/val --batch_size 1 --output_dir keyX --seed X`\n",
    "\n",
    "where X is an integer. Check the output of the load_state_dict method to ensure that there is no error with loading the decoder (incompatible encoder keys and quant_conv are fine). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['encoder.conv_in.weight', 'encoder.conv_in.bias', 'encoder.down.0.block.0.norm1.weight', 'encoder.down.0.block.0.norm1.bias', 'encoder.down.0.block.0.conv1.weight', 'encoder.down.0.block.0.conv1.bias', 'encoder.down.0.block.0.norm2.weight', 'encoder.down.0.block.0.norm2.bias', 'encoder.down.0.block.0.conv2.weight', 'encoder.down.0.block.0.conv2.bias', 'encoder.down.0.block.1.norm1.weight', 'encoder.down.0.block.1.norm1.bias', 'encoder.down.0.block.1.conv1.weight', 'encoder.down.0.block.1.conv1.bias', 'encoder.down.0.block.1.norm2.weight', 'encoder.down.0.block.1.norm2.bias', 'encoder.down.0.block.1.conv2.weight', 'encoder.down.0.block.1.conv2.bias', 'encoder.down.0.downsample.conv.weight', 'encoder.down.0.downsample.conv.bias', 'encoder.down.1.block.0.norm1.weight', 'encoder.down.1.block.0.norm1.bias', 'encoder.down.1.block.0.conv1.weight', 'encoder.down.1.block.0.conv1.bias', 'encoder.down.1.block.0.norm2.weight', 'encoder.down.1.block.0.norm2.bias', 'encoder.down.1.block.0.conv2.weight', 'encoder.down.1.block.0.conv2.bias', 'encoder.down.1.block.0.nin_shortcut.weight', 'encoder.down.1.block.0.nin_shortcut.bias', 'encoder.down.1.block.1.norm1.weight', 'encoder.down.1.block.1.norm1.bias', 'encoder.down.1.block.1.conv1.weight', 'encoder.down.1.block.1.conv1.bias', 'encoder.down.1.block.1.norm2.weight', 'encoder.down.1.block.1.norm2.bias', 'encoder.down.1.block.1.conv2.weight', 'encoder.down.1.block.1.conv2.bias', 'encoder.down.1.downsample.conv.weight', 'encoder.down.1.downsample.conv.bias', 'encoder.down.2.block.0.norm1.weight', 'encoder.down.2.block.0.norm1.bias', 'encoder.down.2.block.0.conv1.weight', 'encoder.down.2.block.0.conv1.bias', 'encoder.down.2.block.0.norm2.weight', 'encoder.down.2.block.0.norm2.bias', 'encoder.down.2.block.0.conv2.weight', 'encoder.down.2.block.0.conv2.bias', 'encoder.down.2.block.0.nin_shortcut.weight', 'encoder.down.2.block.0.nin_shortcut.bias', 'encoder.down.2.block.1.norm1.weight', 'encoder.down.2.block.1.norm1.bias', 'encoder.down.2.block.1.conv1.weight', 'encoder.down.2.block.1.conv1.bias', 'encoder.down.2.block.1.norm2.weight', 'encoder.down.2.block.1.norm2.bias', 'encoder.down.2.block.1.conv2.weight', 'encoder.down.2.block.1.conv2.bias', 'encoder.down.2.downsample.conv.weight', 'encoder.down.2.downsample.conv.bias', 'encoder.down.3.block.0.norm1.weight', 'encoder.down.3.block.0.norm1.bias', 'encoder.down.3.block.0.conv1.weight', 'encoder.down.3.block.0.conv1.bias', 'encoder.down.3.block.0.norm2.weight', 'encoder.down.3.block.0.norm2.bias', 'encoder.down.3.block.0.conv2.weight', 'encoder.down.3.block.0.conv2.bias', 'encoder.down.3.block.1.norm1.weight', 'encoder.down.3.block.1.norm1.bias', 'encoder.down.3.block.1.conv1.weight', 'encoder.down.3.block.1.conv1.bias', 'encoder.down.3.block.1.norm2.weight', 'encoder.down.3.block.1.norm2.bias', 'encoder.down.3.block.1.conv2.weight', 'encoder.down.3.block.1.conv2.bias', 'encoder.mid.block_1.norm1.weight', 'encoder.mid.block_1.norm1.bias', 'encoder.mid.block_1.conv1.weight', 'encoder.mid.block_1.conv1.bias', 'encoder.mid.block_1.norm2.weight', 'encoder.mid.block_1.norm2.bias', 'encoder.mid.block_1.conv2.weight', 'encoder.mid.block_1.conv2.bias', 'encoder.mid.attn_1.norm.weight', 'encoder.mid.attn_1.norm.bias', 'encoder.mid.attn_1.q.weight', 'encoder.mid.attn_1.q.bias', 'encoder.mid.attn_1.k.weight', 'encoder.mid.attn_1.k.bias', 'encoder.mid.attn_1.v.weight', 'encoder.mid.attn_1.v.bias', 'encoder.mid.attn_1.proj_out.weight', 'encoder.mid.attn_1.proj_out.bias', 'encoder.mid.block_2.norm1.weight', 'encoder.mid.block_2.norm1.bias', 'encoder.mid.block_2.conv1.weight', 'encoder.mid.block_2.conv1.bias', 'encoder.mid.block_2.norm2.weight', 'encoder.mid.block_2.norm2.bias', 'encoder.mid.block_2.conv2.weight', 'encoder.mid.block_2.conv2.bias', 'encoder.norm_out.weight', 'encoder.norm_out.bias', 'encoder.conv_out.weight', 'encoder.conv_out.bias', 'quant_conv.weight', 'quant_conv.bias'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "ldm_decoder = deepcopy(ldm_ae)\n",
    "#state_dict = torch.load(\"/ssd/watermarks/stable_signature/sd/sd2_decoder.pth\")\n",
    "state_dict = torch.load(f\"/ssd/watermarks/stable_signature/keys/{key}/checkpoint_000.pth\")['ldm_decoder']\n",
    "\n",
    "msg = ldm_decoder.load_state_dict(state_dict, strict=False)\n",
    "#ldm_decoder.encoder = nn.Identity()\n",
    "#ldm_decoder.quant_conv = nn.Identity()\n",
    "assert(not any([\"decoder\" in x for x in msg.missing_keys]))\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate images\n",
    "\n",
    "Generate images watermarked with a given key. Use the next section on \"Stable Signature - Decoding\" to verify if the watermarked images were correctly generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(img_loader, output_dir, ldm_ae, ldm_decoder, header):\n",
    "    '''\n",
    "    Generate images by: \n",
    "    1. Encoding images using the pretrained stable diffusion encoder ldm_ae\n",
    "    2. Decoding it using the ldm_decoder to generate images with a watermark\n",
    "    3. Decoding it with the original decoder in ldm_ae generate images without \n",
    "       the watermark \n",
    "    4. Apply transformations to augment the original, watermarked, and decoded \n",
    "       (non-watermarked) images\n",
    "    5. Output the watermarked image to output_dir. \n",
    "    \n",
    "    Params: \n",
    "        img_loader: a dataloader loads images from a specified directory\n",
    "        output_dir: directory to output the generated images\n",
    "        ldm_ae: pretrained stable diffusion autoencoder\n",
    "        ldm_decoder: finetuned decoder to generate images with a specific watermark\n",
    "        header: identifier to name images generated by the specific ldm_decoder\n",
    "    '''\n",
    "    ldm_decoder.decoder.eval()\n",
    "    for ii, imgs in enumerate(tqdm.tqdm(img_loader)):\n",
    "        imgs = imgs.to(device)\n",
    "    \n",
    "        imgs_z = ldm_ae.encode(imgs) \n",
    "        imgs_z = imgs_z.mode()\n",
    "\n",
    "        imgs_d0 = ldm_ae.decode(imgs_z) \n",
    "        imgs_w = ldm_decoder.decode(imgs_z) \n",
    "\n",
    "        attacks = {\n",
    "                'none': lambda x: x,\n",
    "                'crop_01': lambda x: utils_img.center_crop(x, 0.1),\n",
    "                'crop_05': lambda x: utils_img.center_crop(x, 0.5),\n",
    "                'rot_25': lambda x: utils_img.rotate(x, 25),\n",
    "                'rot_90': lambda x: utils_img.rotate(x, 90),\n",
    "                'resize_03': lambda x: utils_img.resize(x, 0.3),\n",
    "                'resize_07': lambda x: utils_img.resize(x, 0.7),\n",
    "                'brightness_1p5': lambda x: utils_img.adjust_brightness(x, 1.5),\n",
    "                'brightness_2': lambda x: utils_img.adjust_brightness(x, 2),\n",
    "                'jpeg_80': lambda x: utils_img.jpeg_compress(x, 80),\n",
    "                'jpeg_50': lambda x: utils_img.jpeg_compress(x, 50),\n",
    "                }\n",
    "        for name, attack in attacks.items():\n",
    "            imgs_aug = attack(vqgan_to_imnet(imgs))\n",
    "            save_image(torch.clamp(utils_img.unnormalize_vqgan(imgs_aug),0,1), os.path.join(output_dir, f'{header}_{ii:03}_{name}_orig.png'), nrow=1)\n",
    "            imgs_aug = attack(vqgan_to_imnet(imgs_w))\n",
    "            save_image(torch.clamp(utils_img.unnormalize_vqgan(imgs_aug),0,1), os.path.join(output_dir, f'{header}_{ii:03}_{name}_w.png'), nrow=1)\n",
    "            imgs_aug = attack(vqgan_to_imnet(imgs_d0))\n",
    "            save_image(torch.clamp(utils_img.unnormalize_vqgan(imgs_aug),0,1), os.path.join(output_dir, f'{header}_{ii:03}_{name}_d0.png'), nrow=1)\n",
    "\n",
    "#generate_images(val_loader, \"data/watermarked-val\", ldm_ae, ldm_decoder, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate images with different watermarks \n",
    "\n",
    "Generate the train and val datasets used to train the watermark classifier. Each dataset comprises original images, generated images, and watermarked images of different different keys. Transformations (e.g. rotate, crop) are applied to augment the datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ds(output_dir, start_keys_idx, end_keys_idx, ckpt_path, num_imgs=100):\n",
    "    for i in range(start_keys_idx, end_keys_idx):\n",
    "        key = f\"key{i}\"\n",
    "        key_dir = f'data/{key}'\n",
    "        state_dict = torch.load(f\"/ssd/watermarks/stable_signature/keys/{key}/checkpoint_000.pth\")['ldm_decoder']\n",
    "        msg = ldm_decoder.load_state_dict(state_dict, strict=False)\n",
    "        assert(not any([\"decoder\" in x for x in msg.missing_keys]))\n",
    "        img_loader = utils.get_dataloader(key_dir, vqgan_transform, batch_size, num_imgs=num_imgs, shuffle=True, num_workers=4, collate_fn=None)\n",
    "        generate_images(img_loader, output_dir, ldm_ae, ldm_decoder, key)\n",
    "\n",
    "ckpt_path = f\"/ssd/watermarks/stable_signature/keys/{key}/checkpoint_000.pth\"\n",
    "generate_ds(\"data/watermarked\", 1, 11, ckpt_path)\n",
    "generate_ds(\"data/watermarked-val\", 11, 21, ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Meta Platforms, Inc. and affiliates. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Signature - Decoding\n",
    "\n",
    "Each model has its own key, which can be found in the `keys.txt` file when the fine-tuning is done. \n",
    "The key is a string of 48 bits, which can be converted to a boolean array of 48 elements. \n",
    "The `msg_extractor` is a TorchScript model that extracts the message from the image.\n",
    "\n",
    "Based on the number of matching bits between the key and the message, the image can be classified as genuine or generated by our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruixin/anaconda3/envs/stable-signature/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def msg2str(msg):\n",
    "    return \"\".join([('1' if el else '0') for el in msg])\n",
    "\n",
    "def str2msg(str):\n",
    "    return [True if el=='1' else False for el in str]\n",
    "\n",
    "msg_extractor = torch.jit.load(\"models/dec_48b_whit.torchscript.pt\").to(\"cuda\")\n",
    "transform_imnet = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted message:  001101111100110001100010001100010011000010110011\n"
     ]
    }
   ],
   "source": [
    "# Verify that watermarks were generated correctly \n",
    "img = Image.open(\"/ssd/watermarks/stable_signature/data/watermarked/key1_004_none_w.png\")\n",
    "#img = Image.open(\"/ssd/watermarks/stable_signature/key1/imgs/key1_054_none_w.png\")\n",
    "img = transform_imnet(img).unsqueeze(0).to(\"cuda\")\n",
    "msg = msg_extractor(img) # b c h w -> b k\n",
    "bool_msg = (msg>0).squeeze().cpu().numpy().tolist()\n",
    "print(\"Extracted message: \", msg2str(bool_msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute bit accuracies and run statistical test\n",
    "\n",
    "Metrics are:\n",
    "- **Bit accuracy**: number of matching bits between the key and the message, divided by the total number of bits.\n",
    "- **$p$-value**: probability of observing a bit accuracy as high as the one observed, assuming the null hypothesis that the image is genuine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bit accuracy:  0.9375\n",
      "p-value of statistical test:  BinomTestResult(k=45, n=48, alternative='greater', proportion_estimate=0.9375, pvalue=6.562927978848165e-11)\n"
     ]
    }
   ],
   "source": [
    "#key = '111010110101000001010111010011010100010000100111' # model key\n",
    "key = '001100111100110001100010001100010011000010111111'\n",
    "bool_key = str2msg(key)\n",
    "\n",
    "# compute difference between model key and message extracted from image\n",
    "diff = [bool_msg[i] != bool_key[i] for i in range(len(bool_msg))]\n",
    "bit_acc = 1 - sum(diff)/len(diff)\n",
    "print(\"Bit accuracy: \", bit_acc)\n",
    "\n",
    "# compute p-value\n",
    "from scipy.stats import binomtest\n",
    "pval = binomtest(len(diff)-sum(diff), len(diff), 0.5, alternative='greater')\n",
    "print(\"p-value of statistical test: \", pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, one can set a threshold $\\tau$ on the bit accuracy to classify an image as genuine or generated.\n",
    "Here is the table of FPRs for different thresholds on bit accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import binom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fpr(threshold, n):\n",
    "    return binom.cdf(n-threshold, n, 0.5)\n",
    "\n",
    "thresholds = np.linspace(0, 48, 49)\n",
    "fprs = [fpr(threshold, 48) for threshold in thresholds]\n",
    "\n",
    "plt.plot(thresholds, fprs)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"FPR\")\n",
    "\n",
    "plt.minorticks_on()\n",
    "plt.grid(which='major', linestyle='-', linewidth=0.2, alpha=0.8)\n",
    "plt.grid(axis='both', which='minor', linestyle='-', linewidth=0.1, alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watermark Detector\n",
    "\n",
    "The detector is trained to classify if an image has a watermark that was generated using the stable signature method. Currently only tested on 48-bit watermarks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "batch_size = 8\n",
    "train_dir = \"data/watermarked\"\n",
    "train_size = 33000\n",
    "val_dir = \"data/watermarked-val\"\n",
    "val_size = 1000\n",
    "\n",
    "class ImageFolder:\n",
    "    \"\"\"An image folder dataset intended for supervised learning.\"\"\"\n",
    "\n",
    "    def __init__(self, path, transform=None, loader=default_loader):\n",
    "        self.samples = utils.get_image_paths(path)\n",
    "        self.loader = loader\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Returns the image with its corresponding label. Images are \n",
    "        labeled 0 if the image is not watermarked, else 1. \n",
    "        \"\"\"\n",
    "        assert 0 <= idx < len(self)\n",
    "        path = self.samples[idx]\n",
    "        img = self.loader(path)\n",
    "        label = 0 if \"orig\" in path or \"d0\" in path else 1\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "def collate_fn(data):\n",
    "    tensors, targets = zip(*data)\n",
    "    features = pad_sequence(tensors, batch_first=True)\n",
    "    targets = torch.stack(targets)\n",
    "    return features, targets\n",
    "\n",
    "def get_dataloader(data_dir, transform, batch_size=128, num_imgs=None, shuffle=False, num_workers=4, collate_fn=collate_fn):\n",
    "    \"\"\" Get dataloader for the images in the data_dir. The data_dir must be of the form: input/0/... \"\"\"\n",
    "    dataset = ImageFolder(data_dir, transform=transform)\n",
    "    if num_imgs is not None:\n",
    "        dataset = Subset(dataset, np.random.choice(len(dataset), num_imgs, replace=False))\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True, drop_last=False, collate_fn=collate_fn)\n",
    "\n",
    "train_loader = get_dataloader(train_dir, vqgan_transform, batch_size, num_imgs=train_size, shuffle=True, num_workers=4, collate_fn=None)\n",
    "val_loader = get_dataloader(val_dir, vqgan_transform, 2, num_imgs=val_size, shuffle=True, num_workers=4, collate_fn=None)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for param in ldm_ae.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "ldm_decoder = ldm_decoder.to(\"cpu\")\n",
    "ldm_detector = deepcopy(ldm_decoder)\n",
    "ldm_detector.to(device)\n",
    "ldm_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model\n",
    "\n",
    "Replace the last layer of the [pretrained watermark extractor](https://dl.fbaipublicfiles.com/ssl_watermarking/dec_48b_whit.torchscript.pt) with a fully connected layer comprising one neuron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): RecursiveScriptModule(\n",
       "      original_name=HiddenDecoder\n",
       "      (layers): RecursiveScriptModule(\n",
       "        original_name=Sequential\n",
       "        (0): RecursiveScriptModule(\n",
       "          original_name=ConvBNRelu\n",
       "          (layers): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "            (2): RecursiveScriptModule(original_name=GELU)\n",
       "          )\n",
       "        )\n",
       "        (1): RecursiveScriptModule(\n",
       "          original_name=ConvBNRelu\n",
       "          (layers): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "            (2): RecursiveScriptModule(original_name=GELU)\n",
       "          )\n",
       "        )\n",
       "        (2): RecursiveScriptModule(\n",
       "          original_name=ConvBNRelu\n",
       "          (layers): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "            (2): RecursiveScriptModule(original_name=GELU)\n",
       "          )\n",
       "        )\n",
       "        (3): RecursiveScriptModule(\n",
       "          original_name=ConvBNRelu\n",
       "          (layers): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "            (2): RecursiveScriptModule(original_name=GELU)\n",
       "          )\n",
       "        )\n",
       "        (4): RecursiveScriptModule(\n",
       "          original_name=ConvBNRelu\n",
       "          (layers): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "            (2): RecursiveScriptModule(original_name=GELU)\n",
       "          )\n",
       "        )\n",
       "        (5): RecursiveScriptModule(\n",
       "          original_name=ConvBNRelu\n",
       "          (layers): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "            (2): RecursiveScriptModule(original_name=GELU)\n",
       "          )\n",
       "        )\n",
       "        (6): RecursiveScriptModule(\n",
       "          original_name=ConvBNRelu\n",
       "          (layers): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "            (2): RecursiveScriptModule(original_name=GELU)\n",
       "          )\n",
       "        )\n",
       "        (7): RecursiveScriptModule(\n",
       "          original_name=ConvBNRelu\n",
       "          (layers): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "            (2): RecursiveScriptModule(original_name=GELU)\n",
       "          )\n",
       "        )\n",
       "        (8): RecursiveScriptModule(\n",
       "          original_name=ConvBNRelu\n",
       "          (layers): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "            (2): RecursiveScriptModule(original_name=GELU)\n",
       "          )\n",
       "        )\n",
       "        (9): RecursiveScriptModule(original_name=AdaptiveAvgPool2d)\n",
       "      )\n",
       "      (linear): RecursiveScriptModule(original_name=Linear)\n",
       "    )\n",
       "  )\n",
       "  (1): Dropout(p=0.1, inplace=False)\n",
       "  (2): Linear(in_features=48, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_extractor = torch.jit.load(\"models/dec_48b_whit.torchscript.pt\").to(\"cpu\")\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Sequential(*(list(msg_extractor.children())[:-1])),\n",
    "    nn.Dropout(p=0.1),\n",
    "    nn.Linear(in_features=48, out_features=1)\n",
    ")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, load model if previously saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load(\"models/watermark_classifier.pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def train(model, data_loader, dataset_size, criterion, optimizer, scheduler, epoch):\n",
    "    \"\"\"\n",
    "    Train model using the data_loader for 1 epoch. Prints out a confusion matrix,\n",
    "    loss, and accuracy every 100 steps and at the end of the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    pred_len = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    test_correct = 0\n",
    "\n",
    "    running_loss = 0.0\n",
    "    # Iterate over data.\n",
    "    for step, data in enumerate(tqdm.tqdm(data_loader)):\n",
    "        clear_memory()\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device, dtype=torch.float)\n",
    "        labels = labels.to(device, dtype=torch.float)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs)\n",
    "            targets = labels.unsqueeze(1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            preds = torch.sigmoid(outputs).round().cpu().detach().numpy().squeeze()\n",
    "            y_pred.extend(preds)\n",
    "            labels = labels.cpu().numpy()\n",
    "            y_true.extend(labels)\n",
    "            test_correct += int((preds == labels).sum())\n",
    "            pred_len += preds.size\n",
    "            if step%100 == 0:\n",
    "                print(f'Step: {step}, Loss:  {loss.item():.4f}, Acc: {test_correct/pred_len}')\n",
    "                cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "                print(cf_matrix)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / dataset_size\n",
    "    print(f'Epoch: {epoch}, Loss: {epoch_loss:.4f}, Acc: {test_correct/pred_len}')\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(cf_matrix)\n",
    "    scheduler.step()\n",
    "    return model \n",
    "    \n",
    "def validate(model, data_loader, dataset_size, criterion):\n",
    "    \"\"\"\n",
    "    Validates model using the data_loader. Prints out a confusion matrix,\n",
    "    loss, and accuracy every 100 steps and at the end.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    pred_len = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    test_correct = 0\n",
    "\n",
    "    running_loss = 0.0\n",
    "    # Iterate over data.\n",
    "    for step, d in enumerate(tqdm.tqdm(data_loader)):\n",
    "        clear_memory()\n",
    "        inputs, labels = d\n",
    "        inputs = inputs.to(device, dtype=torch.float)\n",
    "        labels = labels.to(device, dtype=torch.float)\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs)\n",
    "            targets = labels.unsqueeze(1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            preds = torch.sigmoid(outputs).round().cpu().detach().numpy().squeeze()\n",
    "            y_pred.extend(preds)\n",
    "            labels = labels.cpu().numpy()\n",
    "            y_true.extend(labels)\n",
    "            test_correct += int((preds == labels).sum())\n",
    "            pred_len += preds.size\n",
    "            if step%100 == 0:\n",
    "                print(f'Step: {step}, Loss:  {loss.item():.4f}, Acc: {test_correct/pred_len}')\n",
    "                cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "                print(cf_matrix)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / dataset_size\n",
    "    print(f'Val Loss: {epoch_loss:.4f}, Val Acc: {test_correct/pred_len}')\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(cf_matrix)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, train_size, val_loader, val_size, criterion, optimizer, scheduler, num_epochs):\n",
    "    \"\"\"\n",
    "    Runs train and validate for num_epochs. \n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        model = train(model, train_loader, train_size, criterion, optimizer, scheduler, epoch)\n",
    "        clear_memory()\n",
    "        validate(model, val_loader, val_size, criterion)\n",
    "        return model  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                      | 1/4125 [00:00<45:52,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss:  1.6486, Acc: 0.5\n",
      "[[3 2]\n",
      " [2 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|                                                                                                                                                                | 101/4125 [00:44<29:23,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100, Loss:  0.8124, Acc: 0.7871287128712872\n",
      "[[466  76]\n",
      " [ 96 170]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|                                                                                                                                                            | 201/4125 [01:28<29:02,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 200, Loss:  0.0513, Acc: 0.8072139303482587\n",
      "[[929 134]\n",
      " [176 369]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|                                                                                                                                                        | 301/4125 [02:12<28:21,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 300, Loss:  0.4475, Acc: 0.8201827242524917\n",
      "[[1413  186]\n",
      " [ 247  562]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|                                                                                                                                                    | 401/4125 [02:56<27:42,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 400, Loss:  0.3309, Acc: 0.8397755610972568\n",
      "[[1914  211]\n",
      " [ 303  780]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|                                                                                                                                                | 501/4125 [03:41<26:50,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 500, Loss:  0.0409, Acc: 0.8532934131736527\n",
      "[[2423  239]\n",
      " [ 349  997]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|                                                                                                                                            | 601/4125 [04:25<26:33,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 600, Loss:  0.5300, Acc: 0.8612728785357737\n",
      "[[2903  272]\n",
      " [ 395 1238]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|                                                                                                                                        | 701/4125 [05:10<25:29,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 700, Loss:  0.2528, Acc: 0.865549215406562\n",
      "[[3397  309]\n",
      " [ 445 1457]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|                                                                                                                                    | 801/4125 [05:54<24:47,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 800, Loss:  0.1270, Acc: 0.8671972534332085\n",
      "[[3897  344]\n",
      " [ 507 1660]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|                                                                                                                                | 901/4125 [06:39<24:02,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 900, Loss:  0.2779, Acc: 0.8734739178690344\n",
      "[[4429  365]\n",
      " [ 547 1867]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|                                                                                                                           | 1001/4125 [07:23<23:50,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000, Loss:  0.4366, Acc: 0.8778721278721279\n",
      "[[4934  388]\n",
      " [ 590 2096]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|                                                                                                                       | 1101/4125 [08:08<23:05,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1100, Loss:  0.0550, Acc: 0.8815849227974568\n",
      "[[5452  414]\n",
      " [ 629 2313]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|                                                                                                                   | 1201/4125 [08:53<22:05,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1200, Loss:  0.0582, Acc: 0.8859283930058285\n",
      "[[5962  430]\n",
      " [ 666 2550]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|                                                                                                               | 1301/4125 [09:37<21:15,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1300, Loss:  0.0397, Acc: 0.8899884704073789\n",
      "[[6480  444]\n",
      " [ 701 2783]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|                                                                                                           | 1401/4125 [10:22<20:35,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1400, Loss:  0.1752, Acc: 0.89302284082798\n",
      "[[6982  461]\n",
      " [ 738 3027]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|                                                                                                       | 1501/4125 [11:06<19:42,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1500, Loss:  0.5814, Acc: 0.894653564290473\n",
      "[[7505  486]\n",
      " [ 779 3238]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|                                                                                                   | 1601/4125 [11:51<19:14,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1600, Loss:  0.0980, Acc: 0.8977201748906933\n",
      "[[8041  496]\n",
      " [ 814 3457]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|                                                                                               | 1701/4125 [12:36<18:25,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1700, Loss:  0.1238, Acc: 0.9006466784244562\n",
      "[[8526  511]\n",
      " [ 841 3730]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|                                                                                           | 1801/4125 [13:21<17:25,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1800, Loss:  0.0083, Acc: 0.9029705719044975\n",
      "[[9035  526]\n",
      " [ 872 3975]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|                                                                                        | 1901/4125 [14:06<17:02,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1900, Loss:  0.1870, Acc: 0.9058390320883746\n",
      "[[9563  538]\n",
      " [ 894 4213]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|                                                                                    | 2001/4125 [14:51<16:13,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2000, Loss:  0.0184, Acc: 0.907983508245877\n",
      "[[10088   552]\n",
      " [  921  4447]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|                                                                                | 2101/4125 [15:36<15:33,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2100, Loss:  0.0204, Acc: 0.9096263683960019\n",
      "[[10609   568]\n",
      " [  951  4680]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|                                                                            | 2201/4125 [16:20<14:45,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2200, Loss:  0.0037, Acc: 0.9116878691503862\n",
      "[[11112   582]\n",
      " [  973  4941]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|                                                                        | 2301/4125 [17:05<14:05,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2300, Loss:  0.0056, Acc: 0.9141134289439374\n",
      "[[11656   588]\n",
      " [  993  5171]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|                                                                    | 2401/4125 [17:51<13:15,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2400, Loss:  0.0016, Acc: 0.9160766347355269\n",
      "[[12182   596]\n",
      " [ 1016  5414]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|                                                                | 2501/4125 [18:36<13:03,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2500, Loss:  0.1106, Acc: 0.9174330267892843\n",
      "[[12707   610]\n",
      " [ 1042  5649]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|                                                            | 2601/4125 [19:22<11:52,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2600, Loss:  0.1464, Acc: 0.9183006535947712\n",
      "[[13212   629]\n",
      " [ 1071  5896]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|                                                        | 2701/4125 [20:07<11:07,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2700, Loss:  0.0402, Acc: 0.9195205479452054\n",
      "[[13738   639]\n",
      " [ 1100  6131]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|                                                    | 2801/4125 [20:53<10:21,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2800, Loss:  0.0138, Acc: 0.9204302034987505\n",
      "[[14230   651]\n",
      " [ 1132  6395]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|                                                | 2901/4125 [21:38<09:30,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2900, Loss:  0.0571, Acc: 0.9211909686315064\n",
      "[[14741   666]\n",
      " [ 1163  6638]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|                                            | 3001/4125 [22:24<08:48,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3000, Loss:  0.1624, Acc: 0.9217344218593803\n",
      "[[15267   685]\n",
      " [ 1194  6862]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|                                        | 3101/4125 [23:09<07:59,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3100, Loss:  0.0243, Acc: 0.9226459206707514\n",
      "[[15795   696]\n",
      " [ 1223  7094]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|                                    | 3201/4125 [23:54<07:15,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3200, Loss:  0.0238, Acc: 0.9235785691971259\n",
      "[[16320   712]\n",
      " [ 1245  7331]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|                                | 3301/4125 [24:40<06:34,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3300, Loss:  0.1174, Acc: 0.9245683126325356\n",
      "[[16855   720]\n",
      " [ 1272  7561]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|                            | 3401/4125 [25:26<05:39,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3400, Loss:  0.2051, Acc: 0.9260879153190238\n",
      "[[17407   726]\n",
      " [ 1285  7790]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|                        | 3501/4125 [26:11<04:58,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3500, Loss:  0.0047, Acc: 0.9268423307626392\n",
      "[[17929   742]\n",
      " [ 1307  8030]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|                    | 3601/4125 [26:57<04:10,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3600, Loss:  0.1351, Acc: 0.927797833935018\n",
      "[[18473   750]\n",
      " [ 1330  8255]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|                | 3701/4125 [27:43<03:20,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3700, Loss:  0.1616, Acc: 0.9291069981086193\n",
      "[[18998   756]\n",
      " [ 1343  8511]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|            | 3801/4125 [28:29<02:34,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3800, Loss:  0.0005, Acc: 0.9301828466193107\n",
      "[[19522   766]\n",
      " [ 1357  8763]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|        | 3901/4125 [29:14<01:46,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3900, Loss:  0.0138, Acc: 0.931427839015637\n",
      "[[20063   772]\n",
      " [ 1368  9005]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|     | 4001/4125 [30:00<00:59,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4000, Loss:  0.0134, Acc: 0.9319857535616096\n",
      "[[20563   785]\n",
      " [ 1392  9268]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%| | 4101/4125 [30:45<00:11,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4100, Loss:  0.0090, Acc: 0.9319068519873202\n",
      "[[21061   809]\n",
      " [ 1425  9513]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4125/4125 [30:56<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.1849, Acc: 0.931969696969697\n",
      "[[21188   812]\n",
      " [ 1433  9567]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                      | 2/500 [00:00<01:19,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss:  0.1365, Acc: 1.0\n",
      "[[2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|                                                                                                                                   | 102/500 [00:13<00:51,  7.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100, Loss:  2.3708, Acc: 0.9108910891089109\n",
      "[[136  11]\n",
      " [  7  48]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|                                                                                                  | 202/500 [00:26<00:38,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 200, Loss:  0.0158, Acc: 0.8980099502487562\n",
      "[[235  30]\n",
      " [ 11 126]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|                                                                 | 302/500 [00:39<00:25,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 300, Loss:  0.0199, Acc: 0.9053156146179402\n",
      "[[352  42]\n",
      " [ 15 193]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|                                | 402/500 [00:52<00:12,  7.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 400, Loss:  0.0682, Acc: 0.9064837905236908\n",
      "[[469  56]\n",
      " [ 19 258]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 500/500 [01:05<00:00,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.2593, Val Acc: 0.907\n",
      "[[587  70]\n",
      " [ 23 320]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "model = model.to(device)\n",
    "clear_memory()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_sch = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "\n",
    "model = train_model(model, train_loader, train_size, val_loader, val_size, criterion, optimizer_ft, lr_sch, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_model = torch.jit.script(model)\n",
    "torch.jit.save(jit_model, \"models/watermark_classifier.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watermark Remover\n",
    "\n",
    "Create a watermark remover by refining the LDM decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd importort Variable\n",
    "\n",
    "\n",
    "val_dir = 'data/watermarked-val'\n",
    "train_dir = 'data/watermarked'\n",
    "train_size = 22000\n",
    "val_size = 1000\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "class ImageFolder:\n",
    "    \"\"\"An image folder dataset intended for supervised learning.\"\"\"\n",
    "\n",
    "    def __init__(self, path, transform=None, loader=default_loader):\n",
    "        # ignore generated images without a watermark \n",
    "        self.samples = [x for x in utils.get_image_paths(path) if not \"d0\" in x]\n",
    "        self.loader = loader\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        assert 0 <= idx < len(self)\n",
    "        path = self.samples[idx]\n",
    "        \n",
    "        img = None\n",
    "        watermarked_img = None\n",
    "        if \"_orig\" in path:\n",
    "            img = self.loader(path)\n",
    "            path_w = path.replace(\"_orig\", \"_w\")\n",
    "            watermarked_img = self.loader(path_w)\n",
    "\n",
    "        if \"_w\" in path:\n",
    "            path_w = path\n",
    "            path = path.replace(\"_w\", \"_orig\")\n",
    "            watermarked_img = self.loader(path_w)\n",
    "            img = self.loader(path.replace(\"_w\", \"_orig\"))\n",
    "            \n",
    "        if self.transform:\n",
    "            try:\n",
    "                img = self.transform(img)\n",
    "                watermarked_img = self.transform(watermarked_img)\n",
    "                return img, watermarked_img\n",
    "            except: \n",
    "                print(path)\n",
    "                print(path_w)\n",
    "\n",
    "                return img, watermarked_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "def collate_fn(data):\n",
    "    imgs, imgs_w = zip(*data)\n",
    "    imgs = pad_sequence(imgs, batch_first=True)\n",
    "    imgs_w = pad_sequence(imgs_w, batch_first=True)\n",
    "    return imgs, imgs_w\n",
    "\n",
    "def get_dataloader(data_dir, transform, batch_size=128, num_imgs=None, shuffle=False, num_workers=4, collate_fn=collate_fn):\n",
    "    \"\"\" Get dataloader for the images in the data_dir. The data_dir must be of the form: input/0/... \"\"\"\n",
    "    dataset = ImageFolder(data_dir, transform=transform)\n",
    "    if num_imgs is not None:\n",
    "        dataset = Subset(dataset, np.random.choice(len(dataset), num_imgs, replace=False))\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True, drop_last=False, collate_fn=collate_fn)\n",
    "\n",
    "train_loader = get_dataloader(train_dir, vqgan_transform, batch_size, num_imgs=train_size, shuffle=True, num_workers=4, collate_fn=None)\n",
    "val_loader = get_dataloader(val_dir, vqgan_transform, batch_size, num_imgs=val_size, shuffle=True, num_workers=4, collate_fn=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model\n",
    "\n",
    "Define models, loss, criterion, optimizer, and learning rate scheduler. Uses perceptual loss in this case, which is a loss function that measures the difference between the high-level features of two images.\n",
    "Loads existing model in \"models/checkpoint.pt\" and optimizer state if it exists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss.loss_provider import LossProvider\n",
    "provider = LossProvider()\n",
    "loss_percep = provider.get_loss_function('Watson-VGG', colorspace='RGB', pretrained=True, reduction='sum')\n",
    "loss_percep = loss_percep.to(device)\n",
    "loss_i = lambda imgs_w, imgs: loss_percep((1+imgs_w)/2.0, (1+imgs)/2.0)/ imgs_w.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watermark_classifier = torch.jit.load(\"models/watermark_classifier.pt\").to(device)\n",
    "model = deepcopy(ldm_ae)\n",
    "\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in model.decoder.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_sch = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model from checkpoint if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model, optimizer, checkpoint['epoch']\n",
    "\n",
    "model, optimizer, start_epoch = load_ckp(\"models/checkpoint.pt\", model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxrss = 267776\n"
     ]
    }
   ],
   "source": [
    "def debug_memory():\n",
    "    \"\"\"\n",
    "    Helper function to debug GPU memory leak. \n",
    "    \"\"\"\n",
    "    import collections, gc, resource, torch\n",
    "    print('maxrss = {}'.format(\n",
    "        resource.getrusage(resource.RUSAGE_SELF).ru_maxrss))\n",
    "    tensors = collections.Counter(\n",
    "        (str(o.device), str(o.dtype), tuple(o.shape))\n",
    "        for o in gc.get_objects()\n",
    "        if torch.is_tensor(o)\n",
    "    )\n",
    "    for line in sorted(tensors.items()):\n",
    "        print('{}\\t{}'.format(*line))\n",
    "debug_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ckp(state, checkpoint_dir):\n",
    "    f_path = f'{checkpoint_dir}/checkpoint.pt'\n",
    "    torch.save(state, f_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, watermark_classifier, data_loader, dataset_size, criterion, optimizer, scheduler, epoch):\n",
    "    model.decoder.train()\n",
    "    pred_len = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    test_correct = 0\n",
    "    targets = Variable(torch.Tensor(batch_size, 1).fill_(0.0), requires_grad=False).to(device)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    # Iterate over data.\n",
    "    for step, (imgs, imgs_w) in enumerate(tqdm.tqdm(data_loader)):\n",
    "        clear_memory()\n",
    "        imgs = imgs.to(device, dtype=torch.float)\n",
    "        imgs_w = imgs_w.to(device, dtype=torch.float)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            \n",
    "            # encode watermarked images\n",
    "            imgs_z = model.encode(imgs_w) \n",
    "            imgs_z = imgs_z.mode()\n",
    "            #print(torch.cuda.memory_allocated()/ torch.cuda.max_memory_allocated())\n",
    "            outputs = model.decode(imgs_z)\n",
    "            watermark_pred = watermark_classifier(outputs)\n",
    "            # compute loss against non-watermarked imgs\n",
    "            lossi = loss_i(outputs, imgs)\n",
    "            lossw = criterion(watermark_pred, targets)\n",
    "            loss = 0.2 * lossw + 0.8 * lossi\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            preds = torch.sigmoid(watermark_pred).round().cpu().detach().numpy().squeeze()\n",
    "            y_pred.extend(preds)\n",
    "            labels = targets.cpu().numpy().squeeze()\n",
    "            y_true.extend(labels)\n",
    "            test_correct += int((preds == labels).sum())\n",
    "            pred_len += preds.size\n",
    "            if step%100 == 0:\n",
    "                # Save the outpt\n",
    "                print(f'Step: {step}, Loss:  {loss.item():.4f}, Acc: {test_correct/pred_len}')\n",
    "                save_image(torch.clamp(utils_img.unnormalize_vqgan(imgs),0,1), os.path.join(f'{step:03}_train_orig.png'), nrow=8)\n",
    "                save_image(torch.clamp(utils_img.unnormalize_vqgan(outputs),0,1), os.path.join(f'{step:03}_train_removed.png'), nrow=8)\n",
    "                save_image(torch.clamp(utils_img.unnormalize_vqgan(imgs_w),0,1), os.path.join(f'{step:03}_train_w.png'), nrow=8)\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict()\n",
    "                }\n",
    "                save_ckp(checkpoint, \"models/\")\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        del imgs, imgs_w, imgs_z, outputs, watermark_pred\n",
    "    epoch_loss = running_loss / dataset_size\n",
    "    print(f'Epoch: {epoch}, Loss: {epoch_loss:.4f}, Watermark Detection Acc: {test_correct/pred_len}')\n",
    "    scheduler.step()\n",
    "    return model \n",
    "\n",
    "def validate(model, watermark_classifier, data_loader, dataset_size, criterion):\n",
    "    model.decoder.eval()\n",
    "    pred_len = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    test_correct = 0\n",
    "    # Target t\n",
    "    targets = Variable(torch.Tensor(batch_size, 1).fill_(0.0), requires_grad=False).to(device)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    # Iterate over data.\n",
    "    for step, (imgs, imgs_w) in enumerate(tqdm.tqdm(data_loader)):\n",
    "        #print(torch.cuda.memory_allocated()/ torch.cuda.max_memory_allocated())\n",
    "        clear_memory()\n",
    "        imgs = imgs.to(device, dtype=torch.float)\n",
    "        imgs_w = imgs_w.to(device, dtype=torch.float)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "\n",
    "            # encode watermarked images\n",
    "            imgs_z = model.encode(imgs_w) \n",
    "            imgs_z = imgs_z.mode()\n",
    "\n",
    "            outputs = model.decode(imgs_z)\n",
    "            watermark_pred = watermark_classifier(outputs)\n",
    "            \n",
    "            # compute loss against non-watermarked imgs\n",
    "            lossi = loss_i(outputs, imgs)\n",
    "            lossw = criterion(watermark_pred, targets)\n",
    "            loss = 0.2 * lossw + 0.8 * lossi\n",
    "            \n",
    "            preds = torch.sigmoid(watermark_pred).round().cpu().detach().numpy().squeeze()\n",
    "            y_pred.extend(preds)\n",
    "            labels = targets.cpu().numpy().squeeze()\n",
    "            y_true.extend(labels)\n",
    "            test_correct += int((preds == labels).sum())\n",
    "            pred_len += preds.size\n",
    "            if step%100 == 0:\n",
    "                print(f'Step: {step}, Loss:  {loss.item():.4f}, Acc: {test_correct/pred_len}')\n",
    "                save_image(torch.clamp(utils_img.unnormalize_vqgan(imgs),0,1), os.path.join(f'{step:03}_train_orig.png'), nrow=8)\n",
    "                save_image(torch.clamp(utils_img.unnormalize_vqgan(outputs),0,1), os.path.join(f'{step:03}_train_removed.png'), nrow=8)\n",
    "                save_image(torch.clamp(utils_img.unnormalize_vqgan(imgs_w),0,1), os.path.join(f'{step:03}_train_w.png'), nrow=8)\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        del imgs, imgs_w, imgs_z, outputs, watermark_pred\n",
    "    epoch_loss = running_loss / dataset_size\n",
    "    print(f'Loss: {epoch_loss:.4f}, Watermark Detection Acc: {test_correct/pred_len}')\n",
    "    return model \n",
    "\n",
    "def train_model(model, watermark_classifier, train_loader, train_size, val_loader, val_size, criterion, optimizer, scheduler, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        model = train(model, watermark_classifier, train_loader, train_size, criterion, optimizer, scheduler, epoch)\n",
    "        clear_memory()\n",
    "        validate(model, watermark_classifier, val_loader, val_size, criterion)\n",
    "        return model  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                             | 0/11000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss:  5.7798, Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|                                                                                                                                                               | 100/11000 [03:00<3:33:34,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100, Loss:  3.2476, Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|                                                                                                                                                              | 200/11000 [04:59<3:34:45,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 200, Loss:  4.3113, Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|                                                                                                                                                            | 300/11000 [07:00<3:33:39,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 300, Loss:  4.4855, Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|                                                                                                                                                           | 400/11000 [09:02<3:33:58,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 400, Loss:  4.6723, Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|                                                                                                                                                         | 500/11000 [11:05<3:32:51,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 500, Loss:  4.2008, Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|                                                                                                                                                        | 600/11000 [13:08<3:31:07,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 600, Loss:  3.5954, Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|                                                                                                                                                      | 700/11000 [15:11<3:29:14,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 700, Loss:  4.1672, Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|                                                                                                                                                     | 800/11000 [17:14<3:27:17,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 800, Loss:  3.9083, Acc: 0.9993757802746567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|                                                                                                                                                   | 900/11000 [19:18<3:26:40,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 900, Loss:  4.1984, Acc: 0.9994450610432852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|                                                                                                                                                 | 1000/11000 [21:22<3:25:39,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000, Loss:  4.0820, Acc: 0.9995004995004995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|                                                                                                                                                | 1100/11000 [23:26<3:23:23,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1100, Loss:  4.5336, Acc: 0.9995458673932789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|                                                                                                                                              | 1200/11000 [25:31<3:20:47,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1200, Loss:  4.4123, Acc: 0.9991673605328892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|                                                                                                                                             | 1300/11000 [27:35<3:20:37,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1300, Loss:  3.3933, Acc: 0.9992313604919293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|                                                                                                                                           | 1400/11000 [29:40<3:18:04,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1400, Loss:  3.8186, Acc: 0.9992862241256245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|                                                                                                                                          | 1500/11000 [31:45<3:15:17,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1500, Loss:  3.3184, Acc: 0.9993337774816788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|                                                                                                                                        | 1600/11000 [33:50<3:13:56,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1600, Loss:  3.9805, Acc: 0.9993753903810119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|                                                                                                                                       | 1700/11000 [35:55<3:12:31,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1700, Loss:  3.6927, Acc: 0.9994121105232217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|                                                                                                                                     | 1800/11000 [38:01<3:09:47,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1800, Loss:  4.8783, Acc: 0.9994447529150472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|                                                                                                                                    | 1900/11000 [40:07<3:08:43,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1900, Loss:  4.1280, Acc: 0.9994739610731194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|                                                                                                                                   | 2000/11000 [42:12<3:06:10,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2000, Loss:  2.9999, Acc: 0.9995002498750625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|                                                                                                                                 | 2100/11000 [44:18<3:05:30,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2100, Loss:  4.1204, Acc: 0.9995240361732508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|                                                                                                                                | 2200/11000 [46:24<3:03:33,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2200, Loss:  4.0981, Acc: 0.9995456610631531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|                                                                                                                              | 2300/11000 [48:30<3:01:11,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2300, Loss:  3.6931, Acc: 0.9995654063450674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|                                                                                                                             | 2400/11000 [50:36<2:58:50,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2400, Loss:  3.9630, Acc: 0.9995835068721366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|                                                                                                                           | 2500/11000 [52:42<2:56:54,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2500, Loss:  3.9942, Acc: 0.9996001599360256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|                                                                                                                          | 2600/11000 [54:47<2:55:02,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2600, Loss:  3.9084, Acc: 0.9996155324875048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|                                                                                                                        | 2700/11000 [56:53<2:51:47,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2700, Loss:  4.1868, Acc: 0.9996297667530544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|                                                                                                                       | 2800/11000 [58:59<2:50:55,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2800, Loss:  4.9696, Acc: 0.9996429846483399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|                                                                                                                    | 2900/11000 [1:01:05<2:49:07,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2900, Loss:  3.5979, Acc: 0.9996552912788693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|                                                                                                                   | 3000/11000 [1:03:11<2:46:56,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3000, Loss:  3.7001, Acc: 0.9996667777407531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|                                                                                                                 | 3100/11000 [1:05:17<2:44:09,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3100, Loss:  4.1989, Acc: 0.999677523379555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|                                                                                                                | 3200/11000 [1:07:23<2:41:28,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3200, Loss:  4.4524, Acc: 0.999687597625742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|                                                                                                              | 3300/11000 [1:09:29<2:40:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3300, Loss:  3.8434, Acc: 0.9996970614965162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|                                                                                                             | 3400/11000 [1:11:35<2:39:18,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3400, Loss:  4.7734, Acc: 0.9997059688326962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|                                                                                                           | 3500/11000 [1:13:41<2:36:03,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3500, Loss:  3.5004, Acc: 0.9997143673236218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|                                                                                                          | 3600/11000 [1:15:47<2:33:38,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3600, Loss:  4.5056, Acc: 0.9997222993612885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|                                                                                                        | 3700/11000 [1:17:54<2:32:05,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3700, Loss:  4.5311, Acc: 0.9997298027560119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|                                                                                                       | 3800/11000 [1:20:00<2:28:42,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3800, Loss:  3.5795, Acc: 0.9997369113391212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|                                                                                                      | 3900/11000 [1:22:06<2:27:52,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3900, Loss:  3.2039, Acc: 0.9996154832094335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|                                                                                                    | 4000/11000 [1:24:12<2:26:12,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4000, Loss:  4.3036, Acc: 0.9996250937265684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|                                                                                                   | 4100/11000 [1:26:19<2:24:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4100, Loss:  3.7273, Acc: 0.9996342355523044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|                                                                                                 | 4200/11000 [1:28:25<2:21:49,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4200, Loss:  3.5515, Acc: 0.9996429421566294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|                                                                                                | 4300/11000 [1:30:31<2:19:10,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4300, Loss:  3.3848, Acc: 0.9996512438967682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|                                                                                              | 4400/11000 [1:32:37<2:17:09,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4400, Loss:  4.2626, Acc: 0.9996591683708248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|                                                                                             | 4500/11000 [1:34:44<2:15:23,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4500, Loss:  3.3785, Acc: 0.9996667407242835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|                                                                                            | 4600/11000 [1:36:50<2:14:12,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4600, Loss:  3.4835, Acc: 0.9996739839165398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|                                                                                          | 4700/11000 [1:38:56<2:11:13,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4700, Loss:  3.3661, Acc: 0.9996809189534142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|                                                                                         | 4800/11000 [1:41:03<2:09:09,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4800, Loss:  3.9028, Acc: 0.9996875650906061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|                                                                                       | 4900/11000 [1:43:10<2:07:46,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4900, Loss:  3.6487, Acc: 0.9996939400122424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|                                                                                      | 5000/11000 [1:45:17<2:05:05,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5000, Loss:  3.3354, Acc: 0.9997000599880024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|                                                                                    | 5100/11000 [1:47:23<2:03:13,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5100, Loss:  3.5110, Acc: 0.9997059400117624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|                                                                                   | 5200/11000 [1:49:31<2:00:57,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5200, Loss:  3.2118, Acc: 0.9997115939242454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|                                                                                 | 5300/11000 [1:51:38<1:59:45,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5300, Loss:  3.6370, Acc: 0.9997170345217884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|                                                                                | 5400/11000 [1:53:45<1:57:29,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5400, Loss:  3.3337, Acc: 0.9997222736530272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|                                                                               | 5500/11000 [1:55:52<1:55:28,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5500, Loss:  6.5940, Acc: 0.9996364297400473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|                                                                             | 5600/11000 [1:57:59<1:53:18,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5600, Loss:  3.8632, Acc: 0.9995536511337261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|                                                                            | 5700/11000 [2:00:06<1:50:28,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5700, Loss:  3.3756, Acc: 0.9995614804420277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|                                                                          | 5800/11000 [2:02:13<1:48:47,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5800, Loss:  4.2857, Acc: 0.9995690398207205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|                                                                         | 5900/11000 [2:04:21<1:47:12,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5900, Loss:  3.3443, Acc: 0.9995763429927131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|                                                                       | 6000/11000 [2:06:28<1:44:29,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6000, Loss:  3.9887, Acc: 0.9995834027662056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|                                                                      | 6100/11000 [2:08:35<1:42:37,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6100, Loss:  3.6716, Acc: 0.9995902311096542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|                                                                     | 6200/11000 [2:10:42<1:39:59,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6200, Loss:  3.6324, Acc: 0.9995968392194807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|                                                                   | 6300/11000 [2:12:49<1:38:26,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6300, Loss:  3.4604, Acc: 0.9996032375813363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|                                                                  | 6400/11000 [2:14:56<1:36:28,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6400, Loss:  3.7081, Acc: 0.999609436025621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|                                                                | 6500/11000 [2:17:03<1:34:20,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6500, Loss:  4.6602, Acc: 0.9996154437778804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|                                                               | 6600/11000 [2:19:10<1:31:52,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6600, Loss:  3.7158, Acc: 0.9996212695046205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|                                                             | 6700/11000 [2:21:17<1:30:04,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6700, Loss:  3.7511, Acc: 0.9996269213550216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|                                                            | 6800/11000 [2:23:24<1:27:39,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6800, Loss:  2.4110, Acc: 0.9996324069989707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|                                                           | 6900/11000 [2:25:31<1:25:53,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6900, Loss:  4.1107, Acc: 0.9996377336617881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|                                                         | 7000/11000 [2:27:38<1:23:38,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7000, Loss:  3.4568, Acc: 0.9996429081559777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|                                                        | 7100/11000 [2:29:44<1:21:53,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7100, Loss:  3.7379, Acc: 0.9996479369102943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|                                                      | 7200/11000 [2:31:51<1:19:17,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7200, Loss:  3.5595, Acc: 0.9996528259963894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|                                                     | 7300/11000 [2:33:58<1:17:45,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7300, Loss:  2.8412, Acc: 0.9996575811532666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|                                                   | 7400/11000 [2:36:05<1:15:24,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7400, Loss:  3.6352, Acc: 0.9996622078097555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|                                                  | 7500/11000 [2:38:12<1:12:45,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7500, Loss:  4.3217, Acc: 0.999666711105186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|                                                | 7600/11000 [2:40:18<1:11:21,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7600, Loss:  3.6018, Acc: 0.9996053150901197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|                                               | 7700/11000 [2:42:25<1:08:57,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7700, Loss:  4.4057, Acc: 0.9996104402025711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|                                              | 7800/11000 [2:44:32<1:07:06,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7800, Loss:  4.1617, Acc: 0.9996154339187283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|                                            | 7900/11000 [2:46:39<1:04:41,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7900, Loss:  3.3837, Acc: 0.9996203012276927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|                                           | 8000/11000 [2:48:45<1:03:07,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 8000, Loss:  4.2454, Acc: 0.9996250468691413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|                                         | 8100/11000 [2:50:52<1:00:42,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 8100, Loss:  3.9332, Acc: 0.9996296753487224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|                                        | 8200/11000 [2:52:59<58:55,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 8200, Loss:  3.8966, Acc: 0.9996341909523229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|                                       | 8300/11000 [2:55:07<56:31,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 8300, Loss:  3.5189, Acc: 0.9996385977593061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|                                       | 8322/11000 [2:55:37<56:31,  1.27s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m clear_memory()\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwatermark_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_sch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 114\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, watermark_classifier, train_loader, train_size, val_loader, val_size, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, num_epochs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwatermark_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m clear_memory()\n\u001b[1;32m    116\u001b[0m validate(model, watermark_classifier, val_loader, val_size, criterion)\n",
      "Cell \u001b[0;32mIn[15], line 35\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, watermark_classifier, data_loader, dataset_size, criterion, optimizer, scheduler, epoch)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#print(torch.cuda.memory_allocated()/ torch.cuda.max_memory_allocated())\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#debug_memory()\u001b[39;00m\n\u001b[1;32m     34\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 35\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#print(torch.cuda.memory_allocated()/ torch.cuda.max_memory_allocated())\u001b[39;00m\n\u001b[1;32m     38\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(watermark_pred)\u001b[38;5;241m.\u001b[39mround()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/anaconda3/envs/stable-signature/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/stable-signature/lib/python3.8/site-packages/torch/optim/optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/stable-signature/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/stable-signature/lib/python3.8/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/stable-signature/lib/python3.8/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/stable-signature/lib/python3.8/site-packages/torch/optim/adam.py:258\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m step_t\u001b[38;5;241m.\u001b[39mis_cuda, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf capturable=False, state_steps should not be CUDA tensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# update step\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    261\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clear_memory()\n",
    "model = model.to(device)\n",
    "\n",
    "model = train_model(model, watermark_classifier, train_loader, train_size, val_loader, val_size, criterion, optimizer, lr_sch, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable-signature",
   "language": "python",
   "name": "stable-signature"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "df2bc3fed92e5aa2486479205013f8a4acaa6b99dec94d8ee399d56842a5d582"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
