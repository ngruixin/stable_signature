{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Meta Platforms, Inc. and affiliates. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Signature - Decoding\n",
    "\n",
    "Each model has its own key, which can be found in the `keys.txt` file when the fine-tuning is done. \n",
    "The key is a string of 48 bits, which can be converted to a boolean array of 48 elements. \n",
    "The `msg_extractor` is a TorchScript model that extracts the message from the image.\n",
    "\n",
    "Based on the number of matching bits between the key and the message, the image can be classified as genuine or generated by our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def msg2str(msg):\n",
    "    return \"\".join([('1' if el else '0') for el in msg])\n",
    "\n",
    "def str2msg(str):\n",
    "    return [True if el=='1' else False for el in str]\n",
    "\n",
    "msg_extractor = torch.jit.load(\"models/dec_48b_whit.torchscript.pt\").to(\"cuda\")\n",
    "transform_imnet = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted message:  001100111100110001100010001000010011000010110011\n"
     ]
    }
   ],
   "source": [
    "# Verify that watermarks were generated correctly \n",
    "img = Image.open(\"/ssd/watermarks/stable_signature/data/watermarked/key1_004_none_w.png\")\n",
    "#img = Image.open(\"/ssd/watermarks/stable_signature/key1/imgs/key1_054_none_w.png\")\n",
    "img = transform_imnet(img).unsqueeze(0).to(\"cuda\")\n",
    "msg = msg_extractor(img) # b c h w -> b k\n",
    "bool_msg = (msg>0).squeeze().cpu().numpy().tolist()\n",
    "print(\"Extracted message: \", msg2str(bool_msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute bit accuracies and run statistical test\n",
    "\n",
    "Metrics are:\n",
    "- **Bit accuracy**: number of matching bits between the key and the message, divided by the total number of bits.\n",
    "- **$p$-value**: probability of observing a bit accuracy as high as the one observed, assuming the null hypothesis that the image is genuine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bit accuracy:  0.9375\n",
      "p-value of statistical test:  BinomTestResult(k=45, n=48, alternative='greater', proportion_estimate=0.9375, pvalue=6.562927978848165e-11)\n"
     ]
    }
   ],
   "source": [
    "#key = '111010110101000001010111010011010100010000100111' # model key\n",
    "key = '001100111100110001100010001100010011000010111111'\n",
    "bool_key = str2msg(key)\n",
    "\n",
    "# compute difference between model key and message extracted from image\n",
    "diff = [bool_msg[i] != bool_key[i] for i in range(len(bool_msg))]\n",
    "bit_acc = 1 - sum(diff)/len(diff)\n",
    "print(\"Bit accuracy: \", bit_acc)\n",
    "\n",
    "# compute p-value\n",
    "from scipy.stats import binomtest\n",
    "pval = binomtest(len(diff)-sum(diff), len(diff), 0.5, alternative='greater')\n",
    "print(\"p-value of statistical test: \", pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, one can set a threshold $\\tau$ on the bit accuracy to classify an image as genuine or generated.\n",
    "Here is the table of FPRs for different thresholds on bit accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import binom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fpr(threshold, n):\n",
    "    return binom.cdf(n-threshold, n, 0.5)\n",
    "\n",
    "thresholds = np.linspace(0, 48, 49)\n",
    "fprs = [fpr(threshold, 48) for threshold in thresholds]\n",
    "\n",
    "plt.plot(thresholds, fprs)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"FPR\")\n",
    "\n",
    "plt.minorticks_on()\n",
    "plt.grid(which='major', linestyle='-', linewidth=0.2, alpha=0.8)\n",
    "plt.grid(axis='both', which='minor', linestyle='-', linewidth=0.1, alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Watermarked Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "from typing import Callable, Iterable\n",
    "\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import utils\n",
    "import utils_img\n",
    "import utils_model\n",
    "\n",
    "sys.path.append('src')\n",
    "from ldm.models.autoencoder import AutoencoderKL\n",
    "from ldm.models.diffusion.ddpm import LatentDiffusion\n",
    "from loss.loss_provider import LossProvider\n",
    "\n",
    "import gc         # garbage collect library\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    '''\n",
    "    Delete unused tensors on GPU to reduce OOM issues. \n",
    "    '''\n",
    "    try: \n",
    "        del ldm_decoder\n",
    "    except: \n",
    "        pass\n",
    "    try:\n",
    "        del ldm_ae\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        del msg_extractor\n",
    "    except: \n",
    "        pass\n",
    "    try: \n",
    "        del img\n",
    "    except:\n",
    "        pass\n",
    "    try: \n",
    "        del imgs\n",
    "    except:\n",
    "        pass\n",
    "    try: \n",
    "        del targets\n",
    "    except:\n",
    "        pass\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained LDM \n",
    "\n",
    "Load the pretrained Stable Diffusion model [configuration](https://github.com/Stability-AI/stablediffusion/blob/main/configs/stable-diffusion/v2-inference.yaml) and [checkpoint](\n",
    "https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.ckpt). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from sd/v2-1_512-ema-pruned.ckpt\n",
      "Global Step: 220000\n",
      "LatentDiffusion: Running in eps-prediction mode\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m ldm_ckpt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msd/v2-1_512-ema-pruned.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m OmegaConf\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mldm_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m ldm_ae: LatentDiffusion \u001b[38;5;241m=\u001b[39m \u001b[43mutils_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mldm_ckpt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m ldm_ae: AutoencoderKL \u001b[38;5;241m=\u001b[39m ldm_ae\u001b[38;5;241m.\u001b[39mfirst_stage_model\n\u001b[1;32m      6\u001b[0m ldm_ae\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/ssd/watermarks/stable_signature/utils_model.py:140\u001b[0m, in \u001b[0;36mload_model_from_config\u001b[0;34m(config, ckpt, verbose)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGlobal Step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_sd[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_step\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    139\u001b[0m sd \u001b[38;5;241m=\u001b[39m pl_sd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 140\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43minstantiate_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m m, u \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_state_dict(sd, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m verbose:\n",
      "File \u001b[0;32m/ssd/watermarks/stable_signature/utils_model.py:125\u001b[0m, in \u001b[0;36minstantiate_from_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected key `target` to instantiate.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_obj_from_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ssd/watermarks/stable_signature/src/ldm/models/diffusion/ddpm.py:550\u001b[0m, in \u001b[0;36mLatentDiffusion.__init__\u001b[0;34m(self, first_stage_config, cond_stage_config, num_timesteps_cond, cond_stage_key, cond_stage_trainable, concat_mode, cond_stage_forward, conditioning_key, scale_factor, scale_by_std, force_null_conditioning, *args, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m reset_num_ema_updates \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreset_num_ema_updates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    549\u001b[0m ignore_keys \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore_keys\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[0;32m--> 550\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconditioning_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconditioning_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcat_mode \u001b[38;5;241m=\u001b[39m concat_mode\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcond_stage_trainable \u001b[38;5;241m=\u001b[39m cond_stage_trainable\n",
      "File \u001b[0;32m/ssd/watermarks/stable_signature/src/ldm/models/diffusion/ddpm.py:92\u001b[0m, in \u001b[0;36mDDPM.__init__\u001b[0;34m(self, unet_config, timesteps, beta_schedule, loss_type, ckpt_path, ignore_keys, load_only_unet, monitor, use_ema, first_stage_key, image_size, channels, log_every_t, clip_denoised, linear_start, linear_end, cosine_s, given_betas, original_elbo_weight, v_posterior, l_simple_weight, conditioning_key, parameterization, scheduler_config, use_positional_encodings, learn_logvar, logvar_init, make_it_fit, ucg_training, reset_ema, reset_num_ema_updates)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels \u001b[38;5;241m=\u001b[39m channels\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_positional_encodings \u001b[38;5;241m=\u001b[39m use_positional_encodings\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mDiffusionWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43munet_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditioning_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m count_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema \u001b[38;5;241m=\u001b[39m use_ema\n",
      "File \u001b[0;32m/ssd/watermarks/stable_signature/src/ldm/models/diffusion/ddpm.py:1314\u001b[0m, in \u001b[0;36mDiffusionWrapper.__init__\u001b[0;34m(self, diff_model_config, conditioning_key)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequential_cross_attn \u001b[38;5;241m=\u001b[39m diff_model_config\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequential_crossattn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffusion_model \u001b[38;5;241m=\u001b[39m \u001b[43minstantiate_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiff_model_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditioning_key \u001b[38;5;241m=\u001b[39m conditioning_key\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditioning_key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconcat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrossattn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhybrid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhybrid-adm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrossattn-adm\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/ssd/watermarks/stable_signature/src/ldm/util.py:77\u001b[0m, in \u001b[0;36minstantiate_from_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected key `target` to instantiate.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_obj_from_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ssd/watermarks/stable_signature/src/ldm/modules/diffusionmodules/openaimodel.py:589\u001b[0m, in \u001b[0;36mUNetModel.__init__\u001b[0;34m(self, image_size, in_channels, model_channels, out_channels, num_res_blocks, attention_resolutions, dropout, channel_mult, conv_resample, dims, num_classes, use_checkpoint, use_fp16, num_heads, num_head_channels, num_heads_upsample, use_scale_shift_norm, resblock_updown, use_new_attention_order, use_spatial_transformer, transformer_depth, context_dim, n_embed, legacy, disable_self_attentions, num_attention_blocks, disable_middle_self_attn, use_linear_in_transformer)\u001b[0m\n\u001b[1;32m    579\u001b[0m         disabled_sa \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exists(num_attention_blocks) \u001b[38;5;129;01mor\u001b[39;00m nr \u001b[38;5;241m<\u001b[39m num_attention_blocks[level]:\n\u001b[1;32m    582\u001b[0m         layers\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    583\u001b[0m             AttentionBlock(\n\u001b[1;32m    584\u001b[0m                 ch,\n\u001b[1;32m    585\u001b[0m                 use_checkpoint\u001b[38;5;241m=\u001b[39muse_checkpoint,\n\u001b[1;32m    586\u001b[0m                 num_heads\u001b[38;5;241m=\u001b[39mnum_heads,\n\u001b[1;32m    587\u001b[0m                 num_head_channels\u001b[38;5;241m=\u001b[39mdim_head,\n\u001b[1;32m    588\u001b[0m                 use_new_attention_order\u001b[38;5;241m=\u001b[39muse_new_attention_order,\n\u001b[0;32m--> 589\u001b[0m             ) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_spatial_transformer \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mSpatialTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m                \u001b[49m\u001b[43mch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdisable_self_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisabled_sa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_linear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_linear_in_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m                \u001b[49m\u001b[43muse_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_checkpoint\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m         )\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_blocks\u001b[38;5;241m.\u001b[39mappend(TimestepEmbedSequential(\u001b[38;5;241m*\u001b[39mlayers))\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ch\n",
      "File \u001b[0;32m/ssd/watermarks/stable_signature/src/ldm/modules/attention.py:307\u001b[0m, in \u001b[0;36mSpatialTransformer.__init__\u001b[0;34m(self, in_channels, n_heads, d_head, depth, dropout, context_dim, disable_self_attn, use_linear, use_checkpoint)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_in \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(in_channels, inner_dim)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 307\u001b[0m     [BasicTransformerBlock(inner_dim, n_heads, d_head, dropout\u001b[38;5;241m=\u001b[39mdropout, context_dim\u001b[38;5;241m=\u001b[39mcontext_dim[d],\n\u001b[1;32m    308\u001b[0m                            disable_self_attn\u001b[38;5;241m=\u001b[39mdisable_self_attn, checkpoint\u001b[38;5;241m=\u001b[39muse_checkpoint)\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth)]\n\u001b[1;32m    310\u001b[0m )\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_linear:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out \u001b[38;5;241m=\u001b[39m zero_module(nn\u001b[38;5;241m.\u001b[39mConv2d(inner_dim,\n\u001b[1;32m    313\u001b[0m                                           in_channels,\n\u001b[1;32m    314\u001b[0m                                           kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    315\u001b[0m                                           stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    316\u001b[0m                                           padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/ssd/watermarks/stable_signature/src/ldm/modules/attention.py:307\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_in \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(in_channels, inner_dim)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 307\u001b[0m     [\u001b[43mBasicTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_dim\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mdisable_self_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_self_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth)]\n\u001b[1;32m    310\u001b[0m )\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_linear:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out \u001b[38;5;241m=\u001b[39m zero_module(nn\u001b[38;5;241m.\u001b[39mConv2d(inner_dim,\n\u001b[1;32m    313\u001b[0m                                           in_channels,\n\u001b[1;32m    314\u001b[0m                                           kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    315\u001b[0m                                           stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    316\u001b[0m                                           padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/ssd/watermarks/stable_signature/src/ldm/modules/attention.py:260\u001b[0m, in \u001b[0;36mBasicTransformerBlock.__init__\u001b[0;34m(self, dim, n_heads, d_head, dropout, context_dim, gated_ff, checkpoint, disable_self_attn)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_self_attn \u001b[38;5;241m=\u001b[39m disable_self_attn\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn1 \u001b[38;5;241m=\u001b[39m attn_cls(query_dim\u001b[38;5;241m=\u001b[39mdim, heads\u001b[38;5;241m=\u001b[39mn_heads, dim_head\u001b[38;5;241m=\u001b[39md_head, dropout\u001b[38;5;241m=\u001b[39mdropout,\n\u001b[1;32m    259\u001b[0m                       context_dim\u001b[38;5;241m=\u001b[39mcontext_dim \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_self_attn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# is a self-attention if not self.disable_self_attn\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff \u001b[38;5;241m=\u001b[39m \u001b[43mFeedForward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgated_ff\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn2 \u001b[38;5;241m=\u001b[39m attn_cls(query_dim\u001b[38;5;241m=\u001b[39mdim, context_dim\u001b[38;5;241m=\u001b[39mcontext_dim,\n\u001b[1;32m    262\u001b[0m                       heads\u001b[38;5;241m=\u001b[39mn_heads, dim_head\u001b[38;5;241m=\u001b[39md_head, dropout\u001b[38;5;241m=\u001b[39mdropout)  \u001b[38;5;66;03m# is self-attn if context is none\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm(dim)\n",
      "File \u001b[0;32m/ssd/watermarks/stable_signature/src/ldm/modules/attention.py:67\u001b[0m, in \u001b[0;36mFeedForward.__init__\u001b[0;34m(self, dim, dim_out, mult, glu, dropout)\u001b[0m\n\u001b[1;32m     62\u001b[0m inner_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(dim \u001b[38;5;241m*\u001b[39m mult)\n\u001b[1;32m     63\u001b[0m dim_out \u001b[38;5;241m=\u001b[39m default(dim_out, dim)\n\u001b[1;32m     64\u001b[0m project_in \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     65\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(dim, inner_dim),\n\u001b[1;32m     66\u001b[0m     nn\u001b[38;5;241m.\u001b[39mGELU()\n\u001b[0;32m---> 67\u001b[0m ) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m glu \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mGEGLU\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     70\u001b[0m     project_in,\n\u001b[1;32m     71\u001b[0m     nn\u001b[38;5;241m.\u001b[39mDropout(dropout),\n\u001b[1;32m     72\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(inner_dim, dim_out)\n\u001b[1;32m     73\u001b[0m )\n",
      "File \u001b[0;32m/ssd/watermarks/stable_signature/src/ldm/modules/attention.py:52\u001b[0m, in \u001b[0;36mGEGLU.__init__\u001b[0;34m(self, dim_in, dim_out)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dim_in, dim_out):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_out\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/stable-signature/lib/python3.8/site-packages/torch/nn/modules/linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/stable-signature/lib/python3.8/site-packages/torch/nn/modules/linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/anaconda3/envs/stable-signature/lib/python3.8/site-packages/torch/nn/init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    410\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ldm_config = \"sd/v2-inference.yaml\"\n",
    "ldm_ckpt = \"sd/v2-1_512-ema-pruned.ckpt\"\n",
    "config = OmegaConf.load(f\"{ldm_config}\")\n",
    "ldm_ae: LatentDiffusion = utils_model.load_model_from_config(config, ldm_ckpt)\n",
    "ldm_ae: AutoencoderKL = ldm_ae.first_stage_model\n",
    "ldm_ae.eval()\n",
    "ldm_ae.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader\n",
    "\n",
    "Images are loaded from the given directory, then resized (256x256) and cropped to standardize image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "val_dir = 'data/val'\n",
    "train_dir = 'data/train'\n",
    "batch_size = 1     # batch size = 1 to output single images\n",
    "key = \"key20\"\n",
    "key_dir = f'data/{key}'\n",
    "\n",
    "vqgan_transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.CenterCrop(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    utils_img.normalize_vqgan,\n",
    "    ])\n",
    "\n",
    "val_loader = utils.get_dataloader(key_dir, vqgan_transform, batch_size, num_imgs=100, shuffle=False, num_workers=4, collate_fn=None)\n",
    "#train_loader = utils.get_dataloader(train_dir, vqgan_transform, batch_size, num_imgs=500, shuffle=False, num_workers=4, collate_fn=None)\n",
    "#val_loader = utils.get_dataloader(val_dir, vqgan_transform, batch_size, num_imgs=1000, shuffle=False, num_workers=4, collate_fn=None)\n",
    "vqgan_to_imnet = transforms.Compose([utils_img.unnormalize_vqgan, utils_img.normalize_img])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load decoder\n",
    "\n",
    "Copies the pretrained autoencoder and replaces the weights of the decoder with the finetuned decoder that had been generated using the code: \n",
    "\n",
    "`python finetune_ldm_decoder.py --train_dir data/train --val_dir data/val --batch_size 1 --output_dir keyX --seed X`\n",
    "\n",
    "where X is an integer. Check the output of the load_state_dict method to ensure that there is no error with loading the decoder (incompatible encoder keys and quant_conv are fine). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldm_decoder = deepcopy(ldm_ae)\n",
    "#state_dict = torch.load(\"/ssd/watermarks/stable_signature/sd/sd2_decoder.pth\")\n",
    "state_dict = torch.load(f\"/ssd/watermarks/stable_signature/old-keys/{key}/checkpoint_000.pth\")['ldm_decoder']\n",
    "\n",
    "msg = ldm_decoder.load_state_dict(state_dict, strict=False)\n",
    "#ldm_decoder.encoder = nn.Identity()\n",
    "#ldm_decoder.quant_conv = nn.Identity()\n",
    "assert(not any([\"decoder\" in x for x in msg.missing_keys]))\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate images\n",
    "\n",
    "Generate images watermarked with a given key. Use the above section on \"Decoding\" to verify if the watermarked images were correctly generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(img_loader, output_dir, ldm_ae, ldm_decoder, header):\n",
    "    '''\n",
    "    Generate images by: \n",
    "    1. Encoding images using the pretrained stable diffusion encoder ldm_ae\n",
    "    2. Decoding it using the ldm_decoder to generate images with a watermark\n",
    "    3. Decoding it with the original decoder in ldm_ae generate images without \n",
    "       the watermark \n",
    "    4. Apply transformations to augment the original, watermarked, and decoded \n",
    "       (non-watermarked) images\n",
    "    5. Output the watermarked image to output_dir. \n",
    "    \n",
    "    Params: \n",
    "        img_loader: a dataloader loads images from a specified directory\n",
    "        output_dir: directory to output the generated images\n",
    "        ldm_ae: pretrained stable diffusion autoencoder\n",
    "        ldm_decoder: finetuned decoder to generate images with a specific watermark\n",
    "        header: identifier to name images generated by the specific ldm_decoder\n",
    "    '''\n",
    "    ldm_decoder.decoder.eval()\n",
    "    for ii, imgs in enumerate(tqdm.tqdm(img_loader)):\n",
    "        imgs = imgs.to(device)\n",
    "    \n",
    "        imgs_z = ldm_ae.encode(imgs) \n",
    "        imgs_z = imgs_z.mode()\n",
    "\n",
    "        imgs_d0 = ldm_ae.decode(imgs_z) \n",
    "        imgs_w = ldm_decoder.decode(imgs_z) \n",
    "\n",
    "        attacks = {\n",
    "                'none': lambda x: x,\n",
    "                'crop_01': lambda x: utils_img.center_crop(x, 0.1),\n",
    "                'crop_05': lambda x: utils_img.center_crop(x, 0.5),\n",
    "                'rot_25': lambda x: utils_img.rotate(x, 25),\n",
    "                'rot_90': lambda x: utils_img.rotate(x, 90),\n",
    "                'resize_03': lambda x: utils_img.resize(x, 0.3),\n",
    "                'resize_07': lambda x: utils_img.resize(x, 0.7),\n",
    "                'brightness_1p5': lambda x: utils_img.adjust_brightness(x, 1.5),\n",
    "                'brightness_2': lambda x: utils_img.adjust_brightness(x, 2),\n",
    "                'jpeg_80': lambda x: utils_img.jpeg_compress(x, 80),\n",
    "                'jpeg_50': lambda x: utils_img.jpeg_compress(x, 50),\n",
    "                }\n",
    "        for name, attack in attacks.items():\n",
    "            imgs_aug = attack(vqgan_to_imnet(imgs))\n",
    "            save_image(torch.clamp(utils_img.unnormalize_vqgan(imgs_aug),0,1), os.path.join(output_dir, f'{header}_{ii:03}_{name}_orig.png'), nrow=1)\n",
    "            imgs_aug = attack(vqgan_to_imnet(imgs_w))\n",
    "            save_image(torch.clamp(utils_img.unnormalize_vqgan(imgs_aug),0,1), os.path.join(output_dir, f'{header}_{ii:03}_{name}_w.png'), nrow=1)\n",
    "            imgs_aug = attack(vqgan_to_imnet(imgs_d0))\n",
    "            save_image(torch.clamp(utils_img.unnormalize_vqgan(imgs_aug),0,1), os.path.join(output_dir, f'{header}_{ii:03}_{name}_d0.png'), nrow=1)\n",
    "\n",
    "#generate_images(val_loader, \"data/watermarked-val\", ldm_ae, ldm_decoder, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate images with different watermarks \n",
    "\n",
    "Generate the train and val datasets used to train the watermark classifier. Each dataset comprises original images, generated images, and watermarked images of different different keys. Transformations (e.g. rotate, crop) are applied to augment the datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ds(output_dir, start_keys_idx, end_keys_idx, ckpt_path, num_imgs=100):\n",
    "    for i in range(start_keys_idx, end_keys_idx):\n",
    "        key = f\"key{i}\"\n",
    "        key_dir = f'data/{key}'\n",
    "        state_dict = torch.load(f\"/ssd/watermarks/stable_signature/old-keys/{key}/checkpoint_000.pth\")['ldm_decoder']\n",
    "        msg = ldm_decoder.load_state_dict(state_dict, strict=False)\n",
    "        assert(not any([\"decoder\" in x for x in msg.missing_keys]))\n",
    "        img_loader = utils.get_dataloader(key_dir, vqgan_transform, batch_size, num_imgs=num_imgs, shuffle=True, num_workers=4, collate_fn=None)\n",
    "        generate_images(img_loader, output_dir, ldm_ae, ldm_decoder, key)\n",
    "\n",
    "ckpt_path = f\"/ssd/watermarks/stable_signature/old-keys/{key}/checkpoint_000.pth\"\n",
    "generate_ds(\"data/watermarked\", 1, 11, ckpt_path)\n",
    "generate_ds(\"data/watermarked-val\", 11, 21, ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Detector\n",
    "\n",
    "The detector is trained to classify if an image has a watermark that was generated using the stable signature method. Currently only tested on 48-bit watermarks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "batch_size = 8\n",
    "train_dir = \"data/watermarked\"\n",
    "train_size = 1000\n",
    "val_dir = \"data/watermarked-val\"\n",
    "val_size = 23000\n",
    "\n",
    "class ImageFolder:\n",
    "    \"\"\"An image folder dataset intended for supervised learning.\"\"\"\n",
    "\n",
    "    def __init__(self, path, transform=None, loader=default_loader):\n",
    "        self.samples = utils.get_image_paths(path)\n",
    "        self.loader = loader\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Returns the image with its corresponding label. Images are \n",
    "        labeled 0 if the image is not watermarked, else 1. \n",
    "        \"\"\"\n",
    "        assert 0 <= idx < len(self)\n",
    "        path = self.samples[idx]\n",
    "        img = self.loader(path)\n",
    "        label = 0 if \"orig\" in path or \"d0\" in path else 1\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "def collate_fn(data):\n",
    "    tensors, targets = zip(*data)\n",
    "    features = pad_sequence(tensors, batch_first=True)\n",
    "    targets = torch.stack(targets)\n",
    "    return features, targets\n",
    "\n",
    "def get_dataloader(data_dir, transform, batch_size=128, num_imgs=None, shuffle=False, num_workers=4, collate_fn=collate_fn):\n",
    "    \"\"\" Get dataloader for the images in the data_dir. The data_dir must be of the form: input/0/... \"\"\"\n",
    "    dataset = ImageFolder(data_dir, transform=transform)\n",
    "    if num_imgs is not None:\n",
    "        dataset = Subset(dataset, np.random.choice(len(dataset), num_imgs, replace=False))\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True, drop_last=False, collate_fn=collate_fn)\n",
    "\n",
    "train_loader = get_dataloader(train_dir, vqgan_transform, batch_size, num_imgs=train_size, shuffle=True, num_workers=4, collate_fn=None)\n",
    "val_loader = get_dataloader(val_dir, vqgan_transform, batch_size, num_imgs=val_size, shuffle=True, num_workers=4, collate_fn=None)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fc = nn.Linear(48, 1).to(device)\n",
    "\n",
    "for x, y in img_loader:\n",
    "    print(x.size())\n",
    "    print(y)\n",
    "    x = x.to(device)\n",
    "    out = msg_extractor(x) # b c h w -> b z h/f w/f\n",
    "    print(out)\n",
    "    out = fc(out)\n",
    "    print(out.size())\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in ldm_ae.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "ldm_decoder = ldm_decoder.to(\"cpu\")\n",
    "ldm_detector = deepcopy(ldm_decoder)\n",
    "ldm_detector.to(device)\n",
    "ldm_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model\n",
    "\n",
    "Replace the last layer of the [pretrained watermark extractor](https://dl.fbaipublicfiles.com/ssl_watermarking/dec_48b_whit.torchscript.pt) with a fully connected layer comprising one neuron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "msg_extractor = torch.jit.load(\"models/dec_48b_whit.torchscript.pt\").to(\"cpu\")\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Sequential(*(list(msg_extractor.children())[:-1])),\n",
    "    nn.Dropout(p=0.1),\n",
    "    nn.Linear(in_features=48, out_features=1)\n",
    ")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, load model if previously saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load(\"models/watermark_classifier.pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def train(model, data_loader, dataset_size, criterion, optimizer, scheduler, epoch):\n",
    "    \"\"\"\n",
    "    Train model using the data_loader for 1 epoch. Prints out a confusion matrix,\n",
    "    loss, and accuracy every 100 steps and at the end of the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    pred_len = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    test_correct = 0\n",
    "\n",
    "    running_loss = 0.0\n",
    "    # Iterate over data.\n",
    "    for step, data in enumerate(tqdm.tqdm(data_loader)):\n",
    "        clear_memory()\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device, dtype=torch.float)\n",
    "        labels = labels.to(device, dtype=torch.float)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs)\n",
    "            targets = labels.unsqueeze(1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            preds = torch.sigmoid(outputs).round().cpu().detach().numpy().squeeze()\n",
    "            y_pred.extend(preds)\n",
    "            labels = labels.cpu().numpy()\n",
    "            y_true.extend(labels)\n",
    "            test_correct += int((preds == labels).sum())\n",
    "            pred_len += preds.size\n",
    "            if step%100 == 0:\n",
    "                print(f'Step: {step}, Loss:  {loss.item():.4f}, Acc: {test_correct/pred_len}')\n",
    "                cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "                print(cf_matrix)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / dataset_size\n",
    "    print(f'Epoch: {epoch}, Loss: {epoch_loss:.4f}, Acc: {test_correct/pred_len}')\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(cf_matrix)\n",
    "    scheduler.step()\n",
    "    return model \n",
    "    \n",
    "def validate(model, data_loader, dataset_size, criterion):\n",
    "    \"\"\"\n",
    "    Validates model using the data_loader. Prints out a confusion matrix,\n",
    "    loss, and accuracy every 100 steps and at the end.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    pred_len = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    test_correct = 0\n",
    "\n",
    "    running_loss = 0.0\n",
    "    # Iterate over data.\n",
    "    for step, d in enumerate(tqdm.tqdm(data_loader)):\n",
    "        clear_memory()\n",
    "        inputs, labels = d\n",
    "        inputs = inputs.to(device, dtype=torch.float)\n",
    "        labels = labels.to(device, dtype=torch.float)\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs)\n",
    "            targets = labels.unsqueeze(1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            preds = torch.sigmoid(outputs).round().cpu().detach().numpy().squeeze()\n",
    "            y_pred.extend(preds)\n",
    "            labels = labels.cpu().numpy()\n",
    "            y_true.extend(labels)\n",
    "            test_correct += int((preds == labels).sum())\n",
    "            pred_len += preds.size\n",
    "            if step%100 == 0:\n",
    "                print(f'Step: {step}, Loss:  {loss.item():.4f}, Acc: {test_correct/pred_len}')\n",
    "                cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "                print(cf_matrix)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / dataset_size\n",
    "    print(f'Val Loss: {epoch_loss:.4f}, Val Acc: {test_correct/pred_len}')\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(cf_matrix)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, train_size, val_loader, val_size, criterion, optimizer, scheduler, num_epochs):\n",
    "    \"\"\"\n",
    "    Runs train and validate for num_epochs. \n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        model = train(model, train_loader, train_size, criterion, optimizer, scheduler, epoch)\n",
    "        clear_memory()\n",
    "        validate(model, val_loader, val_size, criterion)\n",
    "        return model  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "model = model.to(device)\n",
    "clear_memory()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_sch = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "\n",
    "model = train_model(model, train_loader, train_size, val_loader, val_size, criterion, optimizer_ft, lr_sch, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_model = torch.jit.script(model)\n",
    "torch.jit.save(jit_model, \"models/watermark_classifier.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Watermark Remover\n",
    "\n",
    "Create a watermark remover by refining the LDM decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "val_dir = 'data/watermarked-val'\n",
    "train_dir = 'data/watermarked'\n",
    "train_size = 22000\n",
    "val_size = 1000\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "class ImageFolder:\n",
    "    \"\"\"An image folder dataset intended for supervised learning.\"\"\"\n",
    "\n",
    "    def __init__(self, path, transform=None, loader=default_loader):\n",
    "        # ignore generated images without a watermark \n",
    "        self.samples = [x for x in utils.get_image_paths(path) if not \"d0\" in x]\n",
    "        self.loader = loader\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        assert 0 <= idx < len(self)\n",
    "        path = self.samples[idx]\n",
    "        \n",
    "        img = None\n",
    "        watermarked_img = None\n",
    "        if \"_orig\" in path:\n",
    "            img = self.loader(path)\n",
    "            path_w = path.replace(\"_orig\", \"_w\")\n",
    "            watermarked_img = self.loader(path_w)\n",
    "\n",
    "        if \"_w\" in path:\n",
    "            path_w = path\n",
    "            path = path.replace(\"_w\", \"_orig\")\n",
    "            watermarked_img = self.loader(path_w)\n",
    "            img = self.loader(path.replace(\"_w\", \"_orig\"))\n",
    "            \n",
    "        if self.transform:\n",
    "            try:\n",
    "                img = self.transform(img)\n",
    "                watermarked_img = self.transform(watermarked_img)\n",
    "                return img, watermarked_img\n",
    "            except: \n",
    "                print(path)\n",
    "                print(path_w)\n",
    "\n",
    "                return img, watermarked_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "def collate_fn(data):\n",
    "    imgs, imgs_w = zip(*data)\n",
    "    imgs = pad_sequence(imgs, batch_first=True)\n",
    "    imgs_w = pad_sequence(imgs_w, batch_first=True)\n",
    "    return imgs, imgs_w\n",
    "\n",
    "def get_dataloader(data_dir, transform, batch_size=128, num_imgs=None, shuffle=False, num_workers=4, collate_fn=collate_fn):\n",
    "    \"\"\" Get dataloader for the images in the data_dir. The data_dir must be of the form: input/0/... \"\"\"\n",
    "    dataset = ImageFolder(data_dir, transform=transform)\n",
    "    if num_imgs is not None:\n",
    "        dataset = Subset(dataset, np.random.choice(len(dataset), num_imgs, replace=False))\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True, drop_last=False, collate_fn=collate_fn)\n",
    "\n",
    "train_loader = get_dataloader(train_dir, vqgan_transform, batch_size, num_imgs=train_size, shuffle=True, num_workers=4, collate_fn=None)\n",
    "val_loader = get_dataloader(val_dir, vqgan_transform, batch_size, num_imgs=val_size, shuffle=True, num_workers=4, collate_fn=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model\n",
    "\n",
    "Define models, loss, criterion, optimizer, and learning rate scheduler. Uses perceptual loss in this case, which is a loss function that measures the difference between the high-level features of two images.\n",
    "Loads existing model in \"models/checkpoint.pt\" and optimizer state if it exists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss.loss_provider import LossProvider\n",
    "provider = LossProvider()\n",
    "loss_percep = provider.get_loss_function('Watson-VGG', colorspace='RGB', pretrained=True, reduction='sum')\n",
    "loss_percep = loss_percep.to(device)\n",
    "loss_i = lambda imgs_w, imgs: loss_percep((1+imgs_w)/2.0, (1+imgs)/2.0)/ imgs_w.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watermark_classifier = torch.jit.load(\"models/watermark_classifier.pt\").to(device)\n",
    "model = deepcopy(ldm_ae)\n",
    "\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in model.decoder.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_sch = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return model, optimizer, checkpoint['epoch']\n",
    "\n",
    "model, optimizer, start_epoch = load_ckp(\"models/checkpoint.pt\", model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxrss = 16917284\n",
      "('cpu', 'torch.float32', ())\t139\n",
      "('cpu', 'torch.float32', (2, 3, 256, 256))\t2\n",
      "('cuda:0', 'torch.float32', ())\t6\n",
      "('cuda:0', 'torch.float32', (1,))\t1\n",
      "('cuda:0', 'torch.float32', (1, 3, 1, 1))\t2\n",
      "('cuda:0', 'torch.float32', (1, 48))\t1\n",
      "('cuda:0', 'torch.float32', (3,))\t4\n",
      "('cuda:0', 'torch.float32', (3, 128, 3, 3))\t4\n",
      "('cuda:0', 'torch.float32', (4,))\t2\n",
      "('cuda:0', 'torch.float32', (4, 4, 1, 1))\t2\n",
      "('cuda:0', 'torch.float32', (8,))\t4\n",
      "('cuda:0', 'torch.float32', (8, 8, 1, 1))\t2\n",
      "('cuda:0', 'torch.float32', (8, 512, 3, 3))\t2\n",
      "('cuda:0', 'torch.float32', (48,))\t6\n",
      "('cuda:0', 'torch.float32', (48, 48))\t1\n",
      "('cuda:0', 'torch.float32', (48, 64, 3, 3))\t1\n",
      "('cuda:0', 'torch.float32', (64,))\t43\n",
      "('cuda:0', 'torch.float32', (64, 3, 3, 3))\t2\n",
      "('cuda:0', 'torch.float32', (64, 64, 3, 3))\t8\n",
      "('cuda:0', 'torch.float32', (128,))\t111\n",
      "('cuda:0', 'torch.float32', (128, 3, 3, 3))\t2\n",
      "('cuda:0', 'torch.float32', (128, 64, 3, 3))\t1\n",
      "('cuda:0', 'torch.float32', (128, 128, 3, 3))\t31\n",
      "('cuda:0', 'torch.float32', (128, 256, 1, 1))\t4\n",
      "('cuda:0', 'torch.float32', (128, 256, 3, 3))\t4\n",
      "('cuda:0', 'torch.float32', (256,))\t112\n",
      "('cuda:0', 'torch.float32', (256, 128, 1, 1))\t2\n",
      "('cuda:0', 'torch.float32', (256, 128, 3, 3))\t3\n",
      "('cuda:0', 'torch.float32', (256, 256, 3, 3))\t34\n",
      "('cuda:0', 'torch.float32', (256, 512, 1, 1))\t4\n",
      "('cuda:0', 'torch.float32', (256, 512, 3, 3))\t4\n",
      "('cuda:0', 'torch.float32', (512,))\t332\n",
      "('cuda:0', 'torch.float32', (512, 4, 3, 3))\t4\n",
      "('cuda:0', 'torch.float32', (512, 256, 1, 1))\t2\n",
      "('cuda:0', 'torch.float32', (512, 256, 3, 3))\t3\n",
      "('cuda:0', 'torch.float32', (512, 512, 1, 1))\t24\n",
      "('cuda:0', 'torch.float32', (512, 512, 3, 3))\t101\n",
      "('cuda:0', 'torch.int64', ())\t9\n"
     ]
    }
   ],
   "source": [
    "def debug_memory():\n",
    "    \"\"\"\n",
    "    Helper function to debug GPU memory leak. \n",
    "    \"\"\"\n",
    "    import collections, gc, resource, torch\n",
    "    print('maxrss = {}'.format(\n",
    "        resource.getrusage(resource.RUSAGE_SELF).ru_maxrss))\n",
    "    tensors = collections.Counter(\n",
    "        (str(o.device), str(o.dtype), tuple(o.shape))\n",
    "        for o in gc.get_objects()\n",
    "        if torch.is_tensor(o)\n",
    "    )\n",
    "    for line in sorted(tensors.items()):\n",
    "        print('{}\\t{}'.format(*line))\n",
    "debug_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ckp(state, checkpoint_dir):\n",
    "    f_path = f'{checkpoint_dir}/checkpoint.pt'\n",
    "    torch.save(state, f_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, watermark_classifier, data_loader, dataset_size, criterion, optimizer, scheduler, epoch):\n",
    "    model.decoder.train()\n",
    "    pred_len = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    test_correct = 0\n",
    "    targets = Variable(torch.Tensor(batch_size, 1).fill_(0.0), requires_grad=False).to(device)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    # Iterate over data.\n",
    "    for step, (imgs, imgs_w) in enumerate(tqdm.tqdm(data_loader)):\n",
    "        clear_memory()\n",
    "        imgs = imgs.to(device, dtype=torch.float)\n",
    "        imgs_w = imgs_w.to(device, dtype=torch.float)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            \n",
    "            # encode watermarked images\n",
    "            imgs_z = model.encode(imgs_w) \n",
    "            imgs_z = imgs_z.mode()\n",
    "            #print(torch.cuda.memory_allocated()/ torch.cuda.max_memory_allocated())\n",
    "            outputs = model.decode(imgs_z)\n",
    "            watermark_pred = watermark_classifier(outputs)\n",
    "            #print(torch.cuda.memory_allocated()/ torch.cuda.max_memory_allocated())\n",
    "            # compute loss against non-watermarked imgs\n",
    "            lossi = loss_i(outputs, imgs)\n",
    "            lossw = criterion(watermark_pred, targets)\n",
    "            #print(torch.cuda.memory_allocated()/ torch.cuda.max_memory_allocated())\n",
    "            #debug_memory()\n",
    "            loss = 0.2 * lossw + 0.8 * lossi\n",
    "            #print(torch.cuda.memory_allocated()/ torch.cuda.max_memory_allocated())\n",
    "            #debug_memory()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #print(torch.cuda.memory_allocated()/ torch.cuda.max_memory_allocated())\n",
    "            \n",
    "            preds = torch.sigmoid(watermark_pred).round().cpu().detach().numpy().squeeze()\n",
    "            y_pred.extend(preds)\n",
    "            labels = targets.cpu().numpy().squeeze()\n",
    "            y_true.extend(labels)\n",
    "            test_correct += int((preds == labels).sum())\n",
    "            pred_len += preds.size\n",
    "            if step%100 == 0:\n",
    "                print(f'Step: {step}, Loss:  {loss.item():.4f}, Acc: {test_correct/pred_len}')\n",
    "                save_image(torch.clamp(utils_img.unnormalize_vqgan(imgs),0,1), os.path.join(f'{step:03}_train_orig.png'), nrow=8)\n",
    "                save_image(torch.clamp(utils_img.unnormalize_vqgan(outputs),0,1), os.path.join(f'{step:03}_train_removed.png'), nrow=8)\n",
    "                save_image(torch.clamp(utils_img.unnormalize_vqgan(imgs_w),0,1), os.path.join(f'{step:03}_train_w.png'), nrow=8)\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict()\n",
    "                }\n",
    "                save_ckp(checkpoint, \"models/\")\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        del imgs, imgs_w, imgs_z, outputs, watermark_pred\n",
    "    epoch_loss = running_loss / dataset_size\n",
    "    print(f'Epoch: {epoch}, Loss: {epoch_loss:.4f}, Watermark Detection Acc: {test_correct/pred_len}')\n",
    "    scheduler.step()\n",
    "    return model \n",
    "\n",
    "def validate(model, watermark_classifier, data_loader, dataset_size, criterion):\n",
    "    model.decoder.eval()\n",
    "    pred_len = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    test_correct = 0\n",
    "    targets = Variable(torch.Tensor(batch_size, 1).fill_(0.0), requires_grad=False).to(device)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    # Iterate over data.\n",
    "    for step, (imgs, imgs_w) in enumerate(tqdm.tqdm(data_loader)):\n",
    "        #print(torch.cuda.memory_allocated()/ torch.cuda.max_memory_allocated())\n",
    "        clear_memory()\n",
    "        imgs = imgs.to(device, dtype=torch.float)\n",
    "        imgs_w = imgs_w.to(device, dtype=torch.float)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "\n",
    "            # encode watermarked images\n",
    "            imgs_z = model.encode(imgs_w) \n",
    "            imgs_z = imgs_z.mode()\n",
    "\n",
    "            outputs = model.decode(imgs_z)\n",
    "            watermark_pred = watermark_classifier(outputs)\n",
    "            \n",
    "            # compute loss against non-watermarked imgs\n",
    "            lossi = loss_i(outputs, imgs)\n",
    "            lossw = criterion(watermark_pred, targets)\n",
    "            loss = 0.2 * lossw + 0.8 * lossi\n",
    "            \n",
    "            preds = torch.sigmoid(watermark_pred).round().cpu().detach().numpy().squeeze()\n",
    "            y_pred.extend(preds)\n",
    "            labels = targets.cpu().numpy().squeeze()\n",
    "            y_true.extend(labels)\n",
    "            test_correct += int((preds == labels).sum())\n",
    "            pred_len += preds.size\n",
    "            if step%100 == 0:\n",
    "                print(f'Step: {step}, Loss:  {loss.item():.4f}, Acc: {test_correct/pred_len}')\n",
    "                save_image(torch.clamp(utils_img.unnormalize_vqgan(imgs),0,1), os.path.join(f'{step:03}_train_orig.png'), nrow=8)\n",
    "                save_image(torch.clamp(utils_img.unnormalize_vqgan(outputs),0,1), os.path.join(f'{step:03}_train_removed.png'), nrow=8)\n",
    "                save_image(torch.clamp(utils_img.unnormalize_vqgan(imgs_w),0,1), os.path.join(f'{step:03}_train_w.png'), nrow=8)\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        del imgs, imgs_w, imgs_z, outputs, watermark_pred\n",
    "    epoch_loss = running_loss / dataset_size\n",
    "    print(f'Loss: {epoch_loss:.4f}, Watermark Detection Acc: {test_correct/pred_len}')\n",
    "    return model \n",
    "\n",
    "def train_model(model, watermark_classifier, train_loader, train_size, val_loader, val_size, criterion, optimizer, scheduler, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        model = train(model, watermark_classifier, train_loader, train_size, criterion, optimizer, scheduler, epoch)\n",
    "        clear_memory()\n",
    "        validate(model, watermark_classifier, val_loader, val_size, criterion)\n",
    "        return model  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                             | 0/11000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss:  5.7798, Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|                                                                                                                                                               | 100/11000 [03:00<3:33:34,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100, Loss:  3.2476, Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|                                                                                                                                                              | 200/11000 [04:59<3:34:45,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 200, Loss:  4.3113, Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|                                                                                                                                                            | 300/11000 [07:00<3:33:39,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 300, Loss:  4.4855, Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|                                                                                                                                                           | 400/11000 [09:02<3:33:58,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 400, Loss:  4.6723, Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|                                                                                                                                                         | 500/11000 [11:05<3:32:51,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 500, Loss:  4.2008, Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|                                                                                                                                                        | 600/11000 [13:08<3:31:07,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 600, Loss:  3.5954, Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|                                                                                                                                                      | 700/11000 [15:11<3:29:14,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 700, Loss:  4.1672, Acc: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|                                                                                                                                                     | 800/11000 [17:14<3:27:17,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 800, Loss:  3.9083, Acc: 0.9993757802746567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|                                                                                                                                                   | 900/11000 [19:18<3:26:40,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 900, Loss:  4.1984, Acc: 0.9994450610432852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|                                                                                                                                                 | 1000/11000 [21:22<3:25:39,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000, Loss:  4.0820, Acc: 0.9995004995004995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|                                                                                                                                                | 1100/11000 [23:26<3:23:23,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1100, Loss:  4.5336, Acc: 0.9995458673932789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|                                                                                                                                              | 1200/11000 [25:31<3:20:47,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1200, Loss:  4.4123, Acc: 0.9991673605328892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|                                                                                                                                             | 1300/11000 [27:35<3:20:37,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1300, Loss:  3.3933, Acc: 0.9992313604919293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|                                                                                                                                           | 1400/11000 [29:40<3:18:04,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1400, Loss:  3.8186, Acc: 0.9992862241256245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|                                                                                                                                          | 1500/11000 [31:45<3:15:17,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1500, Loss:  3.3184, Acc: 0.9993337774816788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|                                                                                                                                        | 1600/11000 [33:50<3:13:56,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1600, Loss:  3.9805, Acc: 0.9993753903810119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|                                                                                                                                       | 1700/11000 [35:55<3:12:31,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1700, Loss:  3.6927, Acc: 0.9994121105232217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|                                                                                                                                     | 1800/11000 [38:01<3:09:47,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1800, Loss:  4.8783, Acc: 0.9994447529150472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|                                                                                                                                    | 1900/11000 [40:07<3:08:43,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1900, Loss:  4.1280, Acc: 0.9994739610731194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|                                                                                                                                   | 2000/11000 [42:12<3:06:10,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2000, Loss:  2.9999, Acc: 0.9995002498750625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|                                                                                                                                 | 2100/11000 [44:18<3:05:30,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2100, Loss:  4.1204, Acc: 0.9995240361732508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|                                                                                                                                | 2200/11000 [46:24<3:03:33,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2200, Loss:  4.0981, Acc: 0.9995456610631531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|                                                                                                                              | 2300/11000 [48:30<3:01:11,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2300, Loss:  3.6931, Acc: 0.9995654063450674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|                                                                                                                             | 2400/11000 [50:36<2:58:50,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2400, Loss:  3.9630, Acc: 0.9995835068721366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|                                                                                                                           | 2500/11000 [52:42<2:56:54,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2500, Loss:  3.9942, Acc: 0.9996001599360256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|                                                                                                                          | 2600/11000 [54:47<2:55:02,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2600, Loss:  3.9084, Acc: 0.9996155324875048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|                                                                                                                        | 2700/11000 [56:53<2:51:47,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2700, Loss:  4.1868, Acc: 0.9996297667530544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|                                                                                                                       | 2800/11000 [58:59<2:50:55,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2800, Loss:  4.9696, Acc: 0.9996429846483399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|                                                                                                                    | 2900/11000 [1:01:05<2:49:07,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2900, Loss:  3.5979, Acc: 0.9996552912788693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|                                                                                                                   | 3000/11000 [1:03:11<2:46:56,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3000, Loss:  3.7001, Acc: 0.9996667777407531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|                                                                                                                 | 3100/11000 [1:05:17<2:44:09,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3100, Loss:  4.1989, Acc: 0.999677523379555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|                                                                                                                | 3200/11000 [1:07:23<2:41:28,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3200, Loss:  4.4524, Acc: 0.999687597625742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|                                                                                                              | 3300/11000 [1:09:29<2:40:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3300, Loss:  3.8434, Acc: 0.9996970614965162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|                                                                                                             | 3400/11000 [1:11:35<2:39:18,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3400, Loss:  4.7734, Acc: 0.9997059688326962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|                                                                                                           | 3500/11000 [1:13:41<2:36:03,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3500, Loss:  3.5004, Acc: 0.9997143673236218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|                                                                                                          | 3600/11000 [1:15:47<2:33:38,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3600, Loss:  4.5056, Acc: 0.9997222993612885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|                                                                                                        | 3700/11000 [1:17:54<2:32:05,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3700, Loss:  4.5311, Acc: 0.9997298027560119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|                                                                                                       | 3800/11000 [1:20:00<2:28:42,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3800, Loss:  3.5795, Acc: 0.9997369113391212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|                                                                                                      | 3900/11000 [1:22:06<2:27:52,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3900, Loss:  3.2039, Acc: 0.9996154832094335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|                                                                                                    | 4000/11000 [1:24:12<2:26:12,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4000, Loss:  4.3036, Acc: 0.9996250937265684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|                                                                                                   | 4100/11000 [1:26:19<2:24:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4100, Loss:  3.7273, Acc: 0.9996342355523044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|                                                                                                 | 4200/11000 [1:28:25<2:21:49,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4200, Loss:  3.5515, Acc: 0.9996429421566294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|                                                                                                | 4300/11000 [1:30:31<2:19:10,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4300, Loss:  3.3848, Acc: 0.9996512438967682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|                                                                                              | 4400/11000 [1:32:37<2:17:09,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4400, Loss:  4.2626, Acc: 0.9996591683708248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|                                                                                             | 4500/11000 [1:34:44<2:15:23,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4500, Loss:  3.3785, Acc: 0.9996667407242835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|                                                                                            | 4600/11000 [1:36:50<2:14:12,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4600, Loss:  3.4835, Acc: 0.9996739839165398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|                                                                                          | 4700/11000 [1:38:56<2:11:13,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4700, Loss:  3.3661, Acc: 0.9996809189534142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|                                                                                         | 4800/11000 [1:41:03<2:09:09,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4800, Loss:  3.9028, Acc: 0.9996875650906061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|                                                                                       | 4900/11000 [1:43:10<2:07:46,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4900, Loss:  3.6487, Acc: 0.9996939400122424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|                                                                                      | 5000/11000 [1:45:17<2:05:05,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5000, Loss:  3.3354, Acc: 0.9997000599880024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|                                                                                    | 5100/11000 [1:47:23<2:03:13,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5100, Loss:  3.5110, Acc: 0.9997059400117624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|                                                                                   | 5200/11000 [1:49:31<2:00:57,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5200, Loss:  3.2118, Acc: 0.9997115939242454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|                                                                                 | 5300/11000 [1:51:38<1:59:45,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5300, Loss:  3.6370, Acc: 0.9997170345217884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|                                                                                | 5400/11000 [1:53:45<1:57:29,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5400, Loss:  3.3337, Acc: 0.9997222736530272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|                                                                               | 5500/11000 [1:55:52<1:55:28,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5500, Loss:  6.5940, Acc: 0.9996364297400473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|                                                                             | 5600/11000 [1:57:59<1:53:18,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5600, Loss:  3.8632, Acc: 0.9995536511337261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|                                                                            | 5700/11000 [2:00:06<1:50:28,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5700, Loss:  3.3756, Acc: 0.9995614804420277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|                                                                          | 5800/11000 [2:02:13<1:48:47,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5800, Loss:  4.2857, Acc: 0.9995690398207205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|                                                                         | 5900/11000 [2:04:21<1:47:12,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5900, Loss:  3.3443, Acc: 0.9995763429927131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|                                                                       | 6000/11000 [2:06:28<1:44:29,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6000, Loss:  3.9887, Acc: 0.9995834027662056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|                                                                      | 6100/11000 [2:08:35<1:42:37,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6100, Loss:  3.6716, Acc: 0.9995902311096542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|                                                                     | 6200/11000 [2:10:42<1:39:59,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6200, Loss:  3.6324, Acc: 0.9995968392194807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|                                                                   | 6300/11000 [2:12:49<1:38:26,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6300, Loss:  3.4604, Acc: 0.9996032375813363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|                                                                  | 6400/11000 [2:14:56<1:36:28,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6400, Loss:  3.7081, Acc: 0.999609436025621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|                                                                | 6500/11000 [2:17:03<1:34:20,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6500, Loss:  4.6602, Acc: 0.9996154437778804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|                                                               | 6600/11000 [2:19:10<1:31:52,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6600, Loss:  3.7158, Acc: 0.9996212695046205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|                                                             | 6700/11000 [2:21:17<1:30:04,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6700, Loss:  3.7511, Acc: 0.9996269213550216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|                                                            | 6800/11000 [2:23:24<1:27:39,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6800, Loss:  2.4110, Acc: 0.9996324069989707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|                                                           | 6900/11000 [2:25:31<1:25:53,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6900, Loss:  4.1107, Acc: 0.9996377336617881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|                                                         | 7000/11000 [2:27:38<1:23:38,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7000, Loss:  3.4568, Acc: 0.9996429081559777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|                                                        | 7100/11000 [2:29:44<1:21:53,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7100, Loss:  3.7379, Acc: 0.9996479369102943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|                                                      | 7200/11000 [2:31:51<1:19:17,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7200, Loss:  3.5595, Acc: 0.9996528259963894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|                                                     | 7300/11000 [2:33:58<1:17:45,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7300, Loss:  2.8412, Acc: 0.9996575811532666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|                                                   | 7400/11000 [2:36:05<1:15:24,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7400, Loss:  3.6352, Acc: 0.9996622078097555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|                                                  | 7500/11000 [2:38:12<1:12:45,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7500, Loss:  4.3217, Acc: 0.999666711105186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|                                                | 7600/11000 [2:40:18<1:11:21,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7600, Loss:  3.6018, Acc: 0.9996053150901197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|                                               | 7700/11000 [2:42:25<1:08:57,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7700, Loss:  4.4057, Acc: 0.9996104402025711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|                                              | 7800/11000 [2:44:32<1:07:06,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7800, Loss:  4.1617, Acc: 0.9996154339187283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|                                            | 7900/11000 [2:46:39<1:04:41,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7900, Loss:  3.3837, Acc: 0.9996203012276927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|                                           | 8000/11000 [2:48:45<1:03:07,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 8000, Loss:  4.2454, Acc: 0.9996250468691413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|                                         | 8100/11000 [2:50:52<1:00:42,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 8100, Loss:  3.9332, Acc: 0.9996296753487224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|                                        | 8200/11000 [2:52:59<58:55,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 8200, Loss:  3.8966, Acc: 0.9996341909523229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|                                       | 8300/11000 [2:55:07<56:31,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 8300, Loss:  3.5189, Acc: 0.9996385977593061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|                                       | 8322/11000 [2:55:37<56:31,  1.27s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m clear_memory()\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwatermark_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_sch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 114\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, watermark_classifier, train_loader, train_size, val_loader, val_size, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, num_epochs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwatermark_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m clear_memory()\n\u001b[1;32m    116\u001b[0m validate(model, watermark_classifier, val_loader, val_size, criterion)\n",
      "Cell \u001b[0;32mIn[15], line 35\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, watermark_classifier, data_loader, dataset_size, criterion, optimizer, scheduler, epoch)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#print(torch.cuda.memory_allocated()/ torch.cuda.max_memory_allocated())\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#debug_memory()\u001b[39;00m\n\u001b[1;32m     34\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 35\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#print(torch.cuda.memory_allocated()/ torch.cuda.max_memory_allocated())\u001b[39;00m\n\u001b[1;32m     38\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(watermark_pred)\u001b[38;5;241m.\u001b[39mround()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/anaconda3/envs/stable-signature/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/stable-signature/lib/python3.8/site-packages/torch/optim/optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/stable-signature/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/stable-signature/lib/python3.8/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/stable-signature/lib/python3.8/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/stable-signature/lib/python3.8/site-packages/torch/optim/adam.py:258\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m step_t\u001b[38;5;241m.\u001b[39mis_cuda, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf capturable=False, state_steps should not be CUDA tensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# update step\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    261\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clear_memory()\n",
    "model = model.to(device)\n",
    "\n",
    "model = train_model(model, watermark_classifier, train_loader, train_size, val_loader, val_size, criterion, optimizer, lr_sch, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable-signature",
   "language": "python",
   "name": "stable-signature"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "df2bc3fed92e5aa2486479205013f8a4acaa6b99dec94d8ee399d56842a5d582"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
