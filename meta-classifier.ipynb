{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847c27db-02a5-4c70-a3c5-97e6d71e40f3",
   "metadata": {},
   "source": [
    "# Meta Classifier\n",
    "Detect images generated using Meta AI. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730ae51e-2f7a-410d-a8dc-469f2d8ba786",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee22998a-02f2-4522-abd0-d3461208a6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn as nn\n",
    "\n",
    "import utils_img\n",
    "import utils\n",
    "\n",
    "sys.path.append('src')\n",
    "from loss.loss_provider import LossProvider\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a157b3ca-3b72-46ea-91c9-05ca873f7d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc \n",
    "\n",
    "def clear_memory():\n",
    "    '''\n",
    "    Delete unused tensors on GPU to reduce OOM issues. \n",
    "    '''\n",
    "    try:\n",
    "        del msg_extractor\n",
    "    except: \n",
    "        pass\n",
    "    try: \n",
    "        del img\n",
    "    except:\n",
    "        pass\n",
    "    try: \n",
    "        del imgs\n",
    "    except:\n",
    "        pass\n",
    "    try: \n",
    "        del targets\n",
    "    except:\n",
    "        pass\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e993f-a82c-4c12-a985-2292f74ac3de",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a55d12d2-1539-4655-9b2f-23d00e4b6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "\n",
    "# augment training set with some transformations \n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(img_size),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=180),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.CenterCrop(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6aa64b47-134a-4286-bc26-7206a6943d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np \n",
    "\n",
    "batch_size = 4\n",
    "train_dir = \"data/meta/train\"\n",
    "train_size = 4770\n",
    "val_dir = \"data/meta/val\"\n",
    "val_size = 596\n",
    "\n",
    "class ImageFolder:\n",
    "    \"\"\"An image folder dataset intended for supervised learning.\"\"\"\n",
    "\n",
    "    def __init__(self, path, transform=None, loader=default_loader):\n",
    "        self.samples = [x for x in utils.get_image_paths(path) if not \"aug\" in x]\n",
    "        self.loader = loader\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Returns the image with its corresponding label. Images are \n",
    "        labeled 0 if the image is not watermarked, else 1. \n",
    "        \"\"\"\n",
    "        assert 0 <= idx < len(self)\n",
    "        path = self.samples[idx]\n",
    "        img = self.loader(path)\n",
    "        label = 0 if \"orig\" in path else 1\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "\n",
    "def get_dataloader(data_dir, transform, batch_size=128, num_imgs=None, shuffle=False, num_workers=4):\n",
    "    \"\"\" Get dataloader for the images in the data_dir. The data_dir must be of the form: input/0/... \"\"\"\n",
    "    dataset = ImageFolder(data_dir, transform=transform)\n",
    "    if num_imgs is not None:\n",
    "        dataset = Subset(dataset, np.random.choice(len(dataset), num_imgs, replace=False))\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=True, drop_last=False, collate_fn=None)\n",
    "\n",
    "\n",
    "train_loader = get_dataloader(train_dir, train_transform, batch_size, num_imgs=train_size, shuffle=True)\n",
    "\n",
    "# can try validate with image augmentations or without\n",
    "val_loader = get_dataloader(val_dir, train_transform, 2, num_imgs=val_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2df5583-23cf-4fff-9609-becb2fb33e0d",
   "metadata": {},
   "source": [
    "## Create Model \n",
    "Create a model by finetuning a 64-bit stable signature watermark extractor. Watermark extractor was trained for 300 epochs with the following command:\n",
    "\n",
    "`torchrun --nproc_per_node=8 main.py --local_rank 0 --val_dir /home/test2017/ --train_dir /home/train2017/ --output_dir output64 --eval_freq 5   --img_size 256 --num_bits 64  --batch_size 16 --epochs 300   --scheduler CosineLRScheduler,lr_min=1e-6,t_initial=300,warmup_lr_init=1e-6,warmup_t=5  --optimizer Lamb,lr=2e-2   --p_color_jitter 0.0 --p_blur 0.0 --p_rot 0.0 --p_crop 1.0 --p_res 1.0 --p_jpeg 1.0   --scaling_w 0.3 --scale_channels False --attenuation none   --loss_w_type bce --loss_margin 1 \n",
    "`\n",
    "\n",
    "Watermark extractor was then whitened to ensure that the output bits more independent and well distributed (refer to stable signature paper). A whitened extractor was generated by running the finetune_ldm_decoder.py code once.\n",
    "\n",
    "`python finetune_ldm_decoder.py --train_dir data/train --val_dir data/val --batch_size 1 --output_dir key0 --seed 0`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e2bc81d-53e4-4030-9908-cc9b3bffbaa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): RecursiveScriptModule(\n",
       "      original_name=HiddenDecoder\n",
       "      (layers): RecursiveScriptModule(\n",
       "        original_name=Sequential\n",
       "        (0): RecursiveScriptModule(\n",
       "          original_name=ConvBNRelu\n",
       "          (layers): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "            (2): RecursiveScriptModule(original_name=GELU)\n",
       "          )\n",
       "        )\n",
       "        (1): RecursiveScriptModule(\n",
       "          original_name=ConvBNRelu\n",
       "          (layers): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "            (2): RecursiveScriptModule(original_name=GELU)\n",
       "          )\n",
       "        )\n",
       "        (2): RecursiveScriptModule(\n",
       "          original_name=ConvBNRelu\n",
       "          (layers): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "            (2): RecursiveScriptModule(original_name=GELU)\n",
       "          )\n",
       "        )\n",
       "        (3): RecursiveScriptModule(\n",
       "          original_name=ConvBNRelu\n",
       "          (layers): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "            (2): RecursiveScriptModule(original_name=GELU)\n",
       "          )\n",
       "        )\n",
       "        (4): RecursiveScriptModule(\n",
       "          original_name=ConvBNRelu\n",
       "          (layers): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "            (2): RecursiveScriptModule(original_name=GELU)\n",
       "          )\n",
       "        )\n",
       "        (5): RecursiveScriptModule(\n",
       "          original_name=ConvBNRelu\n",
       "          (layers): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "            (2): RecursiveScriptModule(original_name=GELU)\n",
       "          )\n",
       "        )\n",
       "        (6): RecursiveScriptModule(\n",
       "          original_name=ConvBNRelu\n",
       "          (layers): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "            (2): RecursiveScriptModule(original_name=GELU)\n",
       "          )\n",
       "        )\n",
       "        (7): RecursiveScriptModule(\n",
       "          original_name=ConvBNRelu\n",
       "          (layers): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "            (2): RecursiveScriptModule(original_name=GELU)\n",
       "          )\n",
       "        )\n",
       "        (8): RecursiveScriptModule(\n",
       "          original_name=ConvBNRelu\n",
       "          (layers): RecursiveScriptModule(\n",
       "            original_name=Sequential\n",
       "            (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "            (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "            (2): RecursiveScriptModule(original_name=GELU)\n",
       "          )\n",
       "        )\n",
       "        (9): RecursiveScriptModule(original_name=AdaptiveAvgPool2d)\n",
       "      )\n",
       "      (linear): RecursiveScriptModule(original_name=Linear)\n",
       "    )\n",
       "  )\n",
       "  (1): Dropout(p=0.1, inplace=False)\n",
       "  (2): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_extractor = torch.jit.load(\"hidden/runpod/checkpoint299_whit.pth\").to(\"cpu\")\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Sequential(*(list(msg_extractor.children())[:-1])),\n",
    "    nn.Dropout(p=0.1),\n",
    "    nn.Linear(in_features=64, out_features=1)\n",
    ")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5537eba4-6f2a-4523-b3fe-3374635b505e",
   "metadata": {},
   "source": [
    "## Train Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69003b72-57db-4ce9-a5ca-920629a6cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def train(model, data_loader, dataset_size, criterion, optimizer, scheduler, epoch):\n",
    "    \"\"\"\n",
    "    Train model using the data_loader for 1 epoch. Prints out a confusion matrix,\n",
    "    loss, and accuracy every 100 steps and at the end of the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    pred_len = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    test_correct = 0\n",
    "\n",
    "    running_loss = 0.0\n",
    "    # Iterate over data.\n",
    "    for step, data in enumerate(tqdm.tqdm(data_loader)):\n",
    "        clear_memory()\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device, dtype=torch.float)\n",
    "        labels = labels.to(device, dtype=torch.float)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs)\n",
    "            targets = labels.unsqueeze(1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            preds = torch.sigmoid(outputs).round().cpu().detach().numpy().squeeze()\n",
    "            y_pred.extend(preds)\n",
    "            labels = labels.cpu().numpy()\n",
    "            y_true.extend(labels)\n",
    "            test_correct += int((preds == labels).sum())\n",
    "            pred_len += preds.size\n",
    "            if step%100 == 0:\n",
    "                print(f'Step: {step}, Loss:  {loss.item():.4f}, Acc: {test_correct/pred_len}')\n",
    "                cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "                print(cf_matrix)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / dataset_size\n",
    "    print(f'Epoch: {epoch}, Loss: {epoch_loss:.4f}, Acc: {test_correct/pred_len}')\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(cf_matrix)\n",
    "    scheduler.step()\n",
    "    return model \n",
    "    \n",
    "def validate(model, data_loader, dataset_size, criterion):\n",
    "    \"\"\"\n",
    "    Validates model using the data_loader. Prints out a confusion matrix,\n",
    "    loss, and accuracy every 100 steps and at the end.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    pred_len = 0\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    test_correct = 0\n",
    "\n",
    "    running_loss = 0.0\n",
    "    # Iterate over data.\n",
    "    for step, d in enumerate(tqdm.tqdm(data_loader)):\n",
    "        clear_memory()\n",
    "        inputs, labels = d\n",
    "        inputs = inputs.to(device, dtype=torch.float)\n",
    "        labels = labels.to(device, dtype=torch.float)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            targets = labels.unsqueeze(1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            preds = torch.sigmoid(outputs).round().cpu().detach().numpy().squeeze()\n",
    "            y_pred.extend(preds)\n",
    "            labels = labels.cpu().numpy()\n",
    "            y_true.extend(labels)\n",
    "            test_correct += int((preds == labels).sum())\n",
    "            pred_len += preds.size\n",
    "            if step%100 == 0:\n",
    "                print(f'Step: {step}, Loss:  {loss.item():.4f}, Acc: {test_correct/pred_len}')\n",
    "                cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "                print(cf_matrix)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / dataset_size\n",
    "    print(f'Val Loss: {epoch_loss:.4f}, Val Acc: {test_correct/pred_len}')\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    print(cf_matrix)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, train_size, val_loader, val_size, criterion, optimizer, scheduler, num_epochs):\n",
    "    \"\"\"\n",
    "    Runs train and validate for num_epochs. \n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        model = train(model, train_loader, train_size, criterion, optimizer, scheduler, epoch)\n",
    "        clear_memory()\n",
    "        validate(model, val_loader, val_size, criterion)\n",
    "        return model  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f79c763-5da4-4ace-b28c-0fb15a65b3e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                                                                     | 1/1193 [00:00<08:46,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss:  0.0481, Acc: 1.0\n",
      "[[2 0]\n",
      " [0 2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|█████████████▉                                                                                                                                                      | 101/1193 [00:25<04:34,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100, Loss:  0.0476, Acc: 0.9158415841584159\n",
      "[[175  16]\n",
      " [ 18 195]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|███████████████████████████▋                                                                                                                                        | 201/1193 [00:50<04:12,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 200, Loss:  0.0092, Acc: 0.9154228855721394\n",
      "[[359  33]\n",
      " [ 35 377]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████████████████████████▍                                                                                                                          | 301/1193 [01:15<03:46,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 300, Loss:  0.0482, Acc: 0.9269102990033222\n",
      "[[546  43]\n",
      " [ 45 570]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███████████████████████████████████████████████████████                                                                                                             | 401/1193 [01:41<03:22,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 400, Loss:  0.0165, Acc: 0.9345386533665836\n",
      "[[735  52]\n",
      " [ 53 764]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████████████████████████████████████████████████████████████████████▊                                                                                               | 501/1193 [02:06<02:55,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 500, Loss:  0.0165, Acc: 0.9316367265469062\n",
      "[[933  66]\n",
      " [ 71 934]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████▌                                                                                 | 601/1193 [02:31<02:30,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 600, Loss:  0.0319, Acc: 0.9313643926788685\n",
      "[[1117   80]\n",
      " [  85 1122]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                   | 701/1193 [02:56<02:04,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 700, Loss:  0.1900, Acc: 0.9322396576319544\n",
      "[[1298   92]\n",
      " [  98 1316]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                      | 801/1193 [03:22<01:41,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 800, Loss:  0.0380, Acc: 0.9307116104868914\n",
      "[[1487  111]\n",
      " [ 111 1495]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                        | 901/1193 [03:48<01:15,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 900, Loss:  0.1045, Acc: 0.9311875693673696\n",
      "[[1672  120]\n",
      " [ 128 1684]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                          | 1001/1193 [04:13<00:49,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000, Loss:  0.3705, Acc: 0.9328171828171828\n",
      "[[1872  132]\n",
      " [ 137 1863]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍            | 1101/1193 [04:39<00:23,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1100, Loss:  0.0322, Acc: 0.9332425068119891\n",
      "[[2072  145]\n",
      " [ 149 2038]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1193/1193 [05:02<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.1746, Acc: 0.9335429769392034\n",
      "[[2243  157]\n",
      " [ 160 2210]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▋                                                                                                                                                                     | 3/298 [00:00<00:38,  7.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss:  0.0024, Acc: 1.0\n",
      "[[2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████████████████████████████████████████████████                                                                                                            | 103/298 [00:09<00:15, 12.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100, Loss:  0.3101, Acc: 0.9702970297029703\n",
      "[[109   0]\n",
      " [  6  87]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                    | 203/298 [00:17<00:07, 12.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 200, Loss:  0.4671, Acc: 0.9577114427860697\n",
      "[[206   0]\n",
      " [ 17 179]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 298/298 [00:24<00:00, 11.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1242, Val Acc: 0.9563758389261745\n",
      "[[299   1]\n",
      " [ 25 271]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_sch = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "\n",
    "model = train_model(model, train_loader, train_size, val_loader, val_size, criterion, optimizer_ft, lr_sch, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2742221b-1e1e-4c2e-817d-24df9b464042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▋                                                                                                                                                                     | 3/298 [00:00<00:38,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss:  0.0261, Acc: 1.0\n",
      "[[2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████████████████████████████████████████████████                                                                                                            | 103/298 [00:08<00:16, 12.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100, Loss:  0.0005, Acc: 0.9554455445544554\n",
      "[[ 92   1]\n",
      " [  8 101]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                    | 203/298 [00:16<00:07, 12.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 200, Loss:  2.1426, Acc: 0.9402985074626866\n",
      "[[196   2]\n",
      " [ 22 182]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 298/298 [00:24<00:00, 12.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.1309, Val Acc: 0.9496644295302014\n",
      "[[298   2]\n",
      " [ 28 268]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "validate(model, val_loader, val_size, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97091b0b-eabf-4508-b054-a127410a148d",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "df07973f-f933-473b-94ce-658ffd05014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_model = torch.jit.script(model)\n",
    "torch.jit.save(jit_model, \"models/meta_classifier.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684981fd-7cbf-440b-9390-c806c1c5a4a8",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Code to test on a single image or a test directory. The output 1 indicates that the image is generated by Meta AI, 0 otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea2151ce-e178-4920-915b-47c54ff1bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load(\"models/meta_classifier.pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "110ffd91-7841-4b76-b68b-17fc1bf453d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1., dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "with torch.no_grad():\n",
    "    #img_path = \"/ssd/watermarks/stable_signature/data/meta-aug/331_jpeg_80_w.png\"\n",
    "    img_path = \"/ssd/watermarks/stable_signature/meta4.jpg\"\n",
    "    img = Image.open(img_path)\n",
    "    img = val_transform(img).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    output = model(img)\n",
    "pred = torch.sigmoid(output).round().cpu().detach().numpy().squeeze()\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "423fe08b-6665-46ae-8461-ac4b16069cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██                                                                                                                                                                     | 3/250 [00:00<00:29,  8.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss:  0.0120, Acc: 1.0\n",
      "[[1 0]\n",
      " [0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|███████████████████████████████████████████████████████████████████▉                                                                                                 | 103/250 [00:08<00:11, 12.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100, Loss:  0.0016, Acc: 0.995049504950495\n",
      "[[ 88   0]\n",
      " [  1 113]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                               | 203/250 [00:16<00:03, 12.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 200, Loss:  0.0005, Acc: 0.9925373134328358\n",
      "[[197   0]\n",
      " [  3 202]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:20<00:00, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.0588, Val Acc: 0.994\n",
      "[[250   0]\n",
      " [  3 247]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dir = \"test\"\n",
    "# test with no image augmentations\n",
    "test_loader = get_dataloader(test_dir, val_transform, 2, num_imgs=500, shuffle=True)\n",
    "validate(model, test_loader, val_size, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30dac58-093e-40f2-a0e9-d2b60279974e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable-signature",
   "language": "python",
   "name": "stable-signature"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
